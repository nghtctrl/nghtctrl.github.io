{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Inspirative Text Prediction\n",
        "date: \"2023-12-12\"\n",
        "date-modified: \"2023-12-14\"\n",
        "bibliography: references.bib\n",
        "csl: association-for-computing-machinery.csl\n",
        "execute:\n",
        "  freeze: true\n",
        "  cache: true\n",
        "\n",
        "format:\n",
        "  html:\n",
        "    embed-resources: true\n",
        "    self-contained-math: true\n",
        "---"
      ],
      "id": "0957ad8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Machine learning, and more specifically, deep learning, is shaping how we write. From academic papers to class materials, emails to text messages, we are constantly using technologies powered by deep learning to compose our texts. Moreover, studies have shown that predictive text influences what we write [@Arnold_Chauncey_Gajos_2020; @Arnold_Chauncey_Gajos_2018; @Jakesch_Bhat_Buschek_Zalmanson_Naaman_2023]. Currently, most text prediction technology uses a model that looks at the previously typed words and the surrounding text to generate a list of likely next words or phrases. It ranks each of them based on their probabilities and presents the most likely ones to users as suggestions. However, not only may those suggestions be biased, but they may also affect how users write and what they write, thereby taking away their authorship and autonomy. **Could text prediction models instead serve as a source of _inspiration_ for users, encouraging their writing process instead of suggesting what to write?**\n",
        "\n",
        "In this blog post, I will explore the possibility of using text prediction to _inspire_ users to write more original texts. I will define what it means to be _inspirational_ and then present a preliminary approach to collecting example data and evaluating the current large language models (LLMs) to determine their likelihood of predicting subordinating conjunctions. I will then discuss the challenges and opportunities of using text prediction to _inspire_ users to write more original texts.\n",
        "\n",
        "## Background and Related Work\n",
        "\n",
        "### What does it mean to be _inspirative_?\n",
        "\n",
        "In this blog post, I will use the term inspirative to describe the tendency \"to draw forth or bring out.\"[^1] Specifically, in the context of text prediction, this means that the model should inspire users to write original texts that are not generated by machines. Instead of suggesting the most likely next words or phrases, the model should encourage users to think about what they have written so far and what they could write next. Flower and Hayes define writing as a cognitive process, suggesting that it involves organizing and connecting various types of thinking processes that go into writing, each capable of interrupting and influencing the others [@Flower_Hayes_1981]. How can we support this cognitive process in writing using text prediction? One possible approach investigated by Hyechan Jun, Ha-Ram Koo, and Advait Scaria involves presenting the prediction output in the form of an interview question rather than written text [@Jun_Koo_Scaria_2021]. Questions inherently stimulate thinking and reflection, and this approach has the potential to prompt writers to think about their writing goals and evaluate their written text in order to answer the questions. However, a challenge is that questions must be thought-provoking and they must produce a new concept or idea to warrant their usefulness. In this work, I will explore a different approach. Since text prediction models are already good at predicting texts ahead of time, what if some of those texts could be hidden from the user, inspiring them to complete the text with existing hints?\n",
        "\n",
        "### On Subordinating Conjunctions\n",
        "\n",
        "Suppose a writer writes an independent clause: \"The plant grew taller.\" Instead of suggesting the most likely next words or phrases, the model could predict the word \"because,\" a subordinating conjunction. The word \"because\" has a fascinating property of making the writer think: \"Why did the plant grow taller?\" The writer could then complete the sentence with a dependent clause: \"_because_ they received an adequate amount of sunlight.\" This approach to text prediction has the potential to inspire writers to think about their writing goals and evaluate their written text in order to complete their text with _some_ hint, in this case, a subordinating conjunction. In this work, I will explore this approach by evaluating the current LLMs to determine their likelihood of predicting such subordinating conjunctions.\n",
        "\n",
        "## Exploratory Data Analysis\n"
      ],
      "id": "255817dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import requests\n",
        "import spacy\n",
        "\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "bb11906c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Collection\n",
        "\n",
        "Let's start by defining a function to download a book from [Project Gutenberg](https://www.gutenberg.org/). To accomplish this, we will use [Gutendex](https://gutendex.com/) to retrieve the book's metadata and then download the book using the URL to the plain text version of the book provided in the metadata. For the purpose of this blog post, we will only download books in English.\n"
      ],
      "id": "8a195cf4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: True\n",
        "\n",
        "def download_book(book_id: int) -> tuple[str, str]:\n",
        "    \"\"\"Download a book from Project Gutenberg\n",
        "\n",
        "    Arg:\n",
        "        book_id: The Project Gutenberg ID of the book to download\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the book title and the book text\n",
        "    \"\"\"\n",
        "\n",
        "    gutendex_url = f\"https://gutendex.com/books/{book_id}/\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(gutendex_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        book_language = data[\"languages\"]\n",
        "\n",
        "        # Only download books in English\n",
        "        if \"en\" in book_language:\n",
        "            book_title = data[\"title\"]\n",
        "\n",
        "            # Only download books in plain text\n",
        "            mime_types = [\"text/plain\", \"text/plain; charset=us-ascii\"]\n",
        "\n",
        "            for mime_type in mime_types:\n",
        "                if mime_type in data[\"formats\"]:\n",
        "                    book_url = data[\"formats\"][mime_type]\n",
        "                    break\n",
        "\n",
        "            if book_url is None:\n",
        "                raise Exception(\"The book is not available in plain text.\")\n",
        "\n",
        "            response = requests.get(book_url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            return book_title, response.text\n",
        "        else:\n",
        "            raise Exception(\"The book is not in English.\")\n",
        "    except requests.exceptions.HTTPError as err:\n",
        "        raise Exception(err)"
      ],
      "id": "3a4eacd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this EDA, we will download _The Strange Case of Dr. Jekyll and Mr. Hyde_ by Robert Louis Stevenson.\n"
      ],
      "id": "6278371b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Book ID for The Strange Case of Dr. Jekyll and Mr. Hyde\n",
        "book_id = 43\n",
        "\n",
        "# Download the book and store it in a DataFrame\n",
        "book_data = [download_book(book_id)]\n",
        "book_data = pd.DataFrame(book_data, columns=[\"title\", \"text\"])"
      ],
      "id": "bd7bd013",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrangling\n",
        "\n",
        "Let's take a look at the downloaded text:\n"
      ],
      "id": "8fa0b7b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print the first 256 characters of the book\n",
        "print(book_data[\"text\"].iloc[0][:256].strip() + \"\\n\\n...\\n\")\n",
        "\n",
        "# Print the last 256 characters of the book\n",
        "print(book_data[\"text\"].iloc[0][-256:].strip(), end=\"\")"
      ],
      "id": "149db389",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks like the text contains some extra information which we do not wish to include in our analysis. Let's remove the extra information and save the cleaned text in a new column. \n",
        "\n",
        "Specifically, we will use the markers provided by [Project Gutenberg](https://www.gutenberg.org/) to remove the extra information. These markers appear as follows:\n",
        "\n",
        "> *** START OF THE PROJECT GUTENBERG EBOOK ...\n",
        "\n",
        "> *** END OF THE PROJECT GUTENBERG EBOOK ...\n"
      ],
      "id": "7c85f337"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "def sanitize_text(text: str) -> str:\n",
        "    \"\"\"Remove extra information from the text\n",
        "\n",
        "    Arg:\n",
        "        text: The text to sanitize\n",
        "\n",
        "    Returns:\n",
        "        The sanitized text\n",
        "    \"\"\"\n",
        "\n",
        "    start_marker = \"***\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "    # Index of the second occurrence of the start marker\n",
        "    start_index = text.find(start_marker, text.find(start_marker) + 1)\n",
        "\n",
        "    # Index of the first occurrence of the end marker\n",
        "    end_index = text.find(end_marker)\n",
        "\n",
        "    # Remove the extra information based on the marker indices\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        text = text[start_index + len(start_marker) : end_index].strip()\n",
        "\n",
        "    return text"
      ],
      "id": "555c7c22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanitize the text and store it in a new column\n",
        "book_data[\"clean_text\"] = book_data[\"text\"].apply(sanitize_text)"
      ],
      "id": "b0426b77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the cleaned text:\n"
      ],
      "id": "dffec3be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print the first 256 characters of the book\n",
        "print(book_data[\"clean_text\"].iloc[0][:256].strip() + \"\\n\\n...\\n\")\n",
        "\n",
        "# Print the last 256 characters of the book\n",
        "print(book_data[\"clean_text\"].iloc[0][-256:].strip())"
      ],
      "id": "712dff34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks much better! Our next step is to split the text into sentences to analyze it at the sentence level. We will use [spaCy](https://spacy.io/) to do this:\n"
      ],
      "id": "365abdd2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "def sentence_spliter(text: str) -> list[str]:\n",
        "    \"\"\"Split the text into sentences\n",
        "\n",
        "    Arg:\n",
        "        text: The text to split\n",
        "\n",
        "    Returns:\n",
        "        A list of sentences\n",
        "    \"\"\"\n",
        "\n",
        "    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n",
        "\n",
        "    # Remove line breaks and split the text into sentences\n",
        "    doc = nlp.pipe([text.replace(\"\\r\\n\", \" \")], disable=pipe_disable)\n",
        "\n",
        "    # Return a list of sentences without leading and trailing whitespace\n",
        "    return [sent.text.strip() for doc in doc for sent in doc.sents]"
      ],
      "id": "308db5aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split the text into sentences and store them in a DataFrame\n",
        "sentences = sentence_spliter(book_data[\"clean_text\"].iloc[0])\n",
        "sentences = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "\n",
        "sentences.tail()"
      ],
      "id": "df6e6ba3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many sentences are there in the book?\n"
      ],
      "id": "8b8c200b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "print(f\"There are {len(sentences)} sentences in the book.\")"
      ],
      "id": "b765a30e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many sentence use subordinating conjunctions? In order to answer this question, we will use [spaCy](https://spacy.io/)'s part-of-speech tagger to identify sentences that contain subordinating conjunctions:\n"
      ],
      "id": "1f470763"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "def doc_pipe(sentence: str):\n",
        "    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n",
        "    return list(nlp.pipe([sentence], disable=pipe_disable))\n",
        "\n",
        "\n",
        "def has_sconj(sentence: str):\n",
        "    \"\"\"Check if a sentence contains a subordinating conjunction\n",
        "\n",
        "    Arg:\n",
        "        sentence: The sentence to check\n",
        "\n",
        "    Returns:\n",
        "        A Pandas Series containing a boolean value indicating whether the sentence contains a subordinating conjunction and the subordinating conjunction if it exists\n",
        "    \"\"\"\n",
        "\n",
        "    doc = doc_pipe(sentence)\n",
        "\n",
        "    # Check if the sentence contains a subordinating conjunction\n",
        "    for token in doc[0]:\n",
        "        if token.pos_ == \"SCONJ\":\n",
        "            return pd.Series([True, token.text])\n",
        "\n",
        "    return pd.Series([False, None])"
      ],
      "id": "d1773680",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-overflow: wrap\n",
        "\n",
        "# Check if the sentence contains a subordinating conjunction and store the result in a new column\n",
        "sentences[[\"has_sconj\", \"sconj\"]] = sentences[\"sentence\"].apply(has_sconj)\n",
        "\n",
        "# Sanity check\n",
        "assert sentences[\"has_sconj\"].value_counts().sum() == len(sentences)\n",
        "\n",
        "sentences.tail()"
      ],
      "id": "c2b5b78e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many of the sentences contain subordinating conjunctions? How many of the sentences do not contain subordinating conjunctions?\n"
      ],
      "id": "2974563d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "print(\n",
        "    f\"There are {len(sentences[sentences['has_sconj']])} sentences with a subordinating conjunction,\\nand {len(sentences[~sentences['has_sconj']])} sentences without a subordinating conjunction.\"\n",
        ")"
      ],
      "id": "0df0211e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization\n",
        "\n",
        "Let's try visualizing one of the sentences that contains a subordinating conjunction:\n",
        "\n",
        "**Figure 1.** Visualization of a Sentence That Contains a Subordinating Conjunction\n"
      ],
      "id": "c6e765c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Grab a sentence that contains a subordinating conjunction\n",
        "sentence_id = 1149\n",
        "doc = nlp(sentences[\"sentence\"].iloc[sentence_id])\n",
        "\n",
        "# Visualize the sentence using displaCy\n",
        "spacy.displacy.render(doc, style=\"dep\", jupyter=True, options={\"distance\": 110})"
      ],
      "id": "b3f04dcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What about the distribution of subordinating conjunctions in the book?\n"
      ],
      "id": "1ba252dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Lower case the subordinating conjunctions and count them\n",
        "sent_sconj = sentences[\"sconj\"].str.lower().value_counts().reset_index()\n",
        "\n",
        "# Plot the distribution of subordinating conjunctions\n",
        "fig = px.bar(\n",
        "    sent_sconj,\n",
        "    x=\"sconj\",\n",
        "    y=\"count\",\n",
        "    title=\"<b>Figure 2.</b> Distribution of Subordinating Conjunctions\",\n",
        "    labels={\"sconj\": \"Subordinating Conjunction\", \"count\": \"Count\"},\n",
        "    color_discrete_sequence=px.colors.qualitative.Safe\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "id": "a3689b2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis\n",
        "\n",
        "This result is somewhat surprising to me. I did not expect \"that\" to be the most common subordinating conjunction in the book. I had expected \"because\" to be more common when compared to the other subordinating conjunctions since I personally use \"because\" frequently in my writing. This might suggest that there could be a different distribution of subordinating conjunctions that are more commonly used based on the writing context. Furthermore, this result does not provide any information about which subordinating conjunctions are more useful than others, particularly in the context of text prediction. Our next step is to evaluate the current large language models (LLMs) to determine their likelihood of predicting subordinating conjunctions.\n",
        "\n",
        "## Preliminary Modeling\n"
      ],
      "id": "9816de6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.nn.functional import softmax, cross_entropy\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "b178c0d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Model\n",
        "\n",
        "We will use the [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model to evaluate an LLM's likelihood of predicting subordinating conjunctions. Unfortunately, running the model is computationally expensive on most machines. Therefore, we used [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) to quantize the model into 4-bit precision[^2]. This reduces the amount of computational resources required to run inference on the model while still maintaining a high level of accuracy. We have provided our code for quantizing the model in the Appendix. In the meantime, you can access our quantized model here: [CalvinU/Llama-2-7b-chat-hf-awq](https://huggingface.co/CalvinU/Llama-2-7b-chat-hf-awq).\n"
      ],
      "id": "a32bdffc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_name = \"CalvinU/Llama-2-7b-chat-hf-awq\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "id": "709d3463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Data\n",
        "\n",
        "We have also described in the Appendix a scalable approach to collecting and processing data from [Project Gutenberg](https://www.gutenberg.org/). In the meantime, you can access our dataset here: [CalvinU/project-gutenberg](https://huggingface.co/datasets/CalvinU/project-gutenberg).\n"
      ],
      "id": "48bca22c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"CalvinU/project-gutenberg\"\n",
        "\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset_df = pd.DataFrame(dataset)"
      ],
      "id": "ae91b38f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains 10 random books downloaded from [Project Gutenberg](https://www.gutenberg.org/). These books have already been sanitized and split into sentences based on their `book_id` and `title`. Therefore, each row in the dataset represents an ordered sentence from one of the books. Let's take a look at the dataset:\n"
      ],
      "id": "407b546a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_df.tail()"
      ],
      "id": "4f513200",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrangling\n",
        "\n",
        "Since we already have an ordered list of sentences, we can apply the same approach we used in the EDA section to identify sentences that contain subordinating conjunctions:\n"
      ],
      "id": "45210689"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "def doc_pipe(sentence: str):\n",
        "    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n",
        "    return list(nlp.pipe([sentence], disable=pipe_disable))\n",
        "\n",
        "\n",
        "def has_sconj(sentence: str):\n",
        "    \"\"\"Check if a sentence contains a subordinating conjunction\n",
        "\n",
        "    Arg:\n",
        "        sentence: The sentence to check\n",
        "\n",
        "    Returns:\n",
        "        A Pandas Series containing a boolean value indicating whether the sentence contains a subordinating conjunction and the subordinating conjunction if it exists\n",
        "    \"\"\"\n",
        "\n",
        "    doc = doc_pipe(sentence)\n",
        "\n",
        "    # Check if the sentence contains a subordinating conjunction\n",
        "    for token in doc[0]:\n",
        "        if token.pos_ == \"SCONJ\":\n",
        "            return pd.Series([True, token.text])\n",
        "\n",
        "    return pd.Series([False, None])"
      ],
      "id": "00bacd1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if the sentence contains a subordinating conjunction and store the result in a new column\n",
        "dataset_df[[\"has_sconj\", \"sconj\"]] = dataset_df[\"sentence\"].apply(has_sconj)\n",
        "\n",
        "# Sanity check\n",
        "assert dataset_df[\"has_sconj\"].value_counts().sum() == len(dataset_df)\n",
        "\n",
        "dataset_df.tail()"
      ],
      "id": "35e54c08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of sentences and number of sentences that contain subordinating conjunctions for each book:\n"
      ],
      "id": "2788e361"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary = dataset_df.groupby([\"book_id\", \"title\"], as_index=False).agg(\n",
        "    num_sents=(\"sentence\", \"count\"),\n",
        "    num_sconj=(\"has_sconj\", \"sum\"),\n",
        ")\n",
        "\n",
        "summary.head(10)"
      ],
      "id": "4e1eb1db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis\n",
        "\n",
        "Suppose the general structure of a sentence with a subordinating conjunction is: \n",
        "\n",
        "```\n",
        "<sentence-with-SCONJ> ::= <subordinate-clause> <independent-clause> | \n",
        "                          <independent-clause> <subordinate-clause>\n",
        "```\n",
        "\n",
        "Note that a `<subordinate-clause>` is a _dependent_ clause that contains a subordinating conjunction and cannot stand alone as a sentence, while an `<independent-clause>` is a _main_ clause that can stand alone as a sentence.\n",
        "\n",
        "In order to evaluate the likelihood of an LLM predicting subordinating conjunctions, we will investigate the following behaviors:\n",
        "\n",
        "> How does the cross-entropy and perplexity change when we provide the context exactly as it appears in the book, versus when we randomly shuffle the context?\n",
        "\n",
        "> And for each case, what is the probability spectrum at the subordinating conjunction? What is the cross-entropy and perplexity of the text after the subordinating conjunction?\n",
        "\n",
        "#### When Context Is Provided Exactly as It Appears in the Book\n",
        "\n",
        "To get started, let's select one of the books from the dataset titled _\tThe Adventures of a Dog, and a Good Dog Too_ by Alfred Elwes:\n"
      ],
      "id": "94e67d59"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Book ID for The Adventures of a Dog, and a Good Dog Too\n",
        "book_id = 20741\n",
        "\n",
        "selected_book = dataset_df[dataset_df[\"book_id\"] == book_id].reset_index(drop=True)\n",
        "selected_book.tail()"
      ],
      "id": "84cfad8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many sentences are there in the book? How many of the sentences contain subordinating conjunctions?\n"
      ],
      "id": "0140c156"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "print(\n",
        "    f\"There are {len(selected_book)} sentences in the book. There are {len(selected_book[selected_book['has_sconj']])} sentences with a subordinating conjunction\"\n",
        ")"
      ],
      "id": "072490d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appears that this book uses a significant number of subordinating conjunctions! Let's choose one of the last sentences that includes a subordinating conjunction and select a maximum of 100 sentences preceding it as the context:\n"
      ],
      "id": "65dbd22f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "last_sconj_index = selected_book[selected_book[\"has_sconj\"]].index[-3]\n",
        "\n",
        "context = selected_book.iloc[max(last_sconj_index - 100, 0) : last_sconj_index][\n",
        "    \"sentence\"\n",
        "].tolist()\n",
        "\n",
        "context = \" \".join(context)\n",
        "\n",
        "sentence = selected_book.iloc[last_sconj_index][\"sentence\"]"
      ],
      "id": "bd564443",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a small peek at the context:\n"
      ],
      "id": "b3397aa0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print the first 50 characters of the context\n",
        "print(context[:100].strip() + \" ... \", end=\"\")\n",
        "\n",
        "# Print the last 50 characters of the context\n",
        "print(context[-100:].strip(), end=\"\\n\\n\")"
      ],
      "id": "cc666622",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the sentence with the subordinating conjunction:\n"
      ],
      "id": "43aef31f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print the sentence\n",
        "print(sentence)"
      ],
      "id": "5c9ceaf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's tokenize the context and the sentence, and then feed them into the model to get the predicted logits:\n"
      ],
      "id": "b33c9c86"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Tokenize the context (to be used later, not as an input sequence)\n",
        "context_tokenized = tokenizer(context, return_tensors=\"pt\").to(device)\n",
        "context_input_ids = context_tokenized.input_ids\n",
        "\n",
        "# Tokenize the sentence (to be used later, not as an input sequence)\n",
        "sentence_tokenized = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "sentence_input_ids = sentence_tokenized.input_ids\n",
        "\n",
        "# Tokenize the context and the sentence as an input sequence\n",
        "prompt_tokenized = tokenizer(context + sentence, return_tensors=\"pt\").to(device)\n",
        "prompt_input_ids = prompt_tokenized.input_ids\n",
        "\n",
        "# Get the predicted logits for the input sequence\n",
        "model_logits = model(prompt_input_ids).logits"
      ],
      "id": "06f574eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Get the subordinating conjunction from the book\n",
        "sconj = selected_book.iloc[last_sconj_index][\"sconj\"]\n",
        "\n",
        "# Decode the context as a string, excluding the first token\n",
        "context_decoded = [tokenizer.decode(token) for token in context_input_ids[0]][1:]\n",
        "\n",
        "# Decode the sentence as a string, excluding the first token\n",
        "sentence_decoded = [tokenizer.decode(token) for token in sentence_input_ids[0]][1:]\n",
        "\n",
        "# Index of the subordinating conjunction token in the input sequence\n",
        "sconj_token_index = len(context_decoded) + sentence_decoded.index(sconj)\n",
        "\n",
        "# Index of the subordinating conjunction in the sentence (not the input sequence, but from the book)\n",
        "sconj_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(sconj)\n",
        "\n",
        "# Figure out which type of clause comes first\n",
        "if sconj not in sentence[:sconj_index]:\n",
        "    independent_clause = sentence[:sconj_index]\n",
        "    subordinate_clause = sentence[sconj_index:]\n",
        "else:\n",
        "    independent_clause = sentence[sconj_index:]\n",
        "    subordinate_clause = sentence[:sconj_index]\n",
        "\n",
        "# Index of the independent clause in the sentence (not the input sequence, but from the book)\n",
        "independent_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n",
        "    independent_clause\n",
        ")\n",
        "\n",
        "# Index of the subordinate clause in the sentence (not the input sequence, but from the book)\n",
        "subordinate_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n",
        "    subordinate_clause\n",
        ")\n",
        "\n",
        "# Tokenize the independent clause\n",
        "independent_clause_tokenized = tokenizer(\n",
        "    independent_clause, return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "# Tokenize the subordinate clause\n",
        "subordinate_clause_tokenized = tokenizer(\n",
        "    subordinate_clause, return_tensors=\"pt\"\n",
        ").to(device)"
      ],
      "id": "b55b82a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given that we have fed in the context exactly as it appears in the book, let's take a look at the top k probability spectrum at the subordinating conjunction:\n"
      ],
      "id": "bdc5e768"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "def probability_spectrum_at(logits, input_ids, i, k=6):\n",
        "    \"\"\"Given an input sequence, get the top k probability spectrum at the given index\n",
        "\n",
        "    Args:\n",
        "        logits: predicted logits for the input sequence\n",
        "        input_ids: input sequence token IDs\n",
        "        i: index to get the probability spectrum at\n",
        "        k: top k, default is 6\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame containing the top k probability spectrum at the given index\n",
        "    \"\"\"\n",
        "\n",
        "    # Predicted logits for an input sequence, excluding the last element\n",
        "    adjusted_logits = logits[0, :-1]\n",
        "\n",
        "    # Input sequence, starting from the second element\n",
        "    adjusted_input_ids = input_ids[0, 1:]\n",
        "\n",
        "    # Get the probability distribution predicted by the model\n",
        "    probability_distribution = softmax(adjusted_logits[i], dim=0)\n",
        "\n",
        "    # Get the top k probabilities and their respective indices, default k=6\n",
        "    top_probability_distribution, top_indices = probability_distribution.topk(k)\n",
        "\n",
        "    # Get the top k probability spectrum as a DataFrame\n",
        "    probability_spectrum = pd.DataFrame(\n",
        "        {\n",
        "            \"token\": [tokenizer.decode(token) for token in top_indices.tolist()],\n",
        "            \"probability\": top_probability_distribution.tolist(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Decode the input sequence as a string\n",
        "    matching_token = tokenizer.decode(adjusted_input_ids[i])\n",
        "\n",
        "    # Highlight the matching string in the probability spectrum\n",
        "    def highlight_prompt_at(x):\n",
        "        if x[\"token\"] == matching_token:\n",
        "            return [\"background-color: #6495ED\"] * len(x)\n",
        "        else:\n",
        "            return [\"\"] * len(x)\n",
        "\n",
        "    return probability_spectrum.style.apply(highlight_prompt_at, axis=1)\n",
        "\n",
        "\n",
        "def cross_entropy_at(logits, input_ids, i):\n",
        "    \"\"\"Given an input sequence, get the cross entropy at the given index\n",
        "\n",
        "    Args:\n",
        "        logits: predicted logits for the input sequence\n",
        "        input_ids: input sequence token IDs\n",
        "        i: index to get the cross entropy at\n",
        "\n",
        "    Returns:\n",
        "        The cross entropy at the given index\n",
        "    \"\"\"\n",
        "\n",
        "    # Predicted logits for an input sequence, excluding the last element\n",
        "    adjusted_logits = logits[0, :-1]\n",
        "\n",
        "    # Input sequence, starting from the second element\n",
        "    adjusted_input_ids = input_ids[0, 1:]\n",
        "\n",
        "    # Get the cross entropy per input sequence\n",
        "    cross_entropy_seq = cross_entropy(\n",
        "        adjusted_logits, adjusted_input_ids, reduction=\"none\"\n",
        "    )\n",
        "\n",
        "    return cross_entropy_seq[i].item()\n",
        "\n",
        "\n",
        "def cross_entropy_per_token(logits, input_ids, matching_sequence_tokenized):\n",
        "    \"\"\"Given a matching sequence, get the cross entropy for each token in the matching sequence\n",
        "\n",
        "    Args:\n",
        "        logits: predicted logits for the input sequence\n",
        "        input_ids: input sequence token IDs\n",
        "        matching_sequence_tokenized: tokenized matching sequence\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame containing the cross entropy for each token in the matching sequence\n",
        "    \"\"\"\n",
        "\n",
        "    # Predicted logits for an input sequence, excluding the last element\n",
        "    adjusted_logits = logits[0, :-1]\n",
        "\n",
        "    # Input sequence, starting from the second element\n",
        "    adjusted_input_ids = input_ids[0, 1:]\n",
        "\n",
        "    # Get the cross entropy per input sequence\n",
        "    cross_entropy_seq = cross_entropy(\n",
        "        adjusted_logits, adjusted_input_ids, reduction=\"none\"\n",
        "    )\n",
        "\n",
        "    # Decode the tokenized matching sequence as a string\n",
        "    matching_sequence_token = [\n",
        "        tokenizer.decode(token) for token in matching_sequence_tokenized.input_ids[0]\n",
        "    ]\n",
        "\n",
        "    # Decoded matching sequence token, starting from the second element\n",
        "    adjusted_matching_sequence_token = matching_sequence_token[1:]\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"token\": adjusted_matching_sequence_token,\n",
        "            \"cross_entropy\": cross_entropy_seq[\n",
        "                -len(adjusted_matching_sequence_token) :\n",
        "            ].tolist(),\n",
        "        }\n",
        "    )"
      ],
      "id": "d57d121b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "prob_spectrum = probability_spectrum_at(\n",
        "    model_logits, prompt_input_ids, sconj_token_index\n",
        ")\n",
        "\n",
        "prob_spectrum"
      ],
      "id": "86523f0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also look at the cross-entropy and perplexity of the sentence with the subordinating conjunction:\n"
      ],
      "id": "e914c356"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\n",
        "sentence_per_token_cross_entropy = cross_entropy_per_token(\n",
        "    model_logits, prompt_input_ids, sentence_tokenized\n",
        ")\n",
        "\n",
        "# Mean cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\n",
        "mean_sentence_cross_entropy = sentence_per_token_cross_entropy[\"cross_entropy\"].mean()\n",
        "\n",
        "# Perplexity of the sentence with the subordinating conjunction (entire input sequence)\n",
        "sentence_perplexity = np.exp(mean_sentence_cross_entropy)\n",
        "\n",
        "# Cross-entropy of the independent clause\n",
        "independent_clause_per_token_cross_entropy = cross_entropy_per_token(\n",
        "    model_logits, prompt_input_ids, independent_clause_tokenized\n",
        ")\n",
        "\n",
        "# Mean cross-entropy of the independent clause\n",
        "mean_independent_clause_cross_entropy = independent_clause_per_token_cross_entropy[\n",
        "    \"cross_entropy\"\n",
        "].mean()\n",
        "\n",
        "# Perplexity of the independent clause\n",
        "independent_clause_perplexity = np.exp(mean_independent_clause_cross_entropy)\n",
        "\n",
        "# Cross-entropy of the subordinate clause\n",
        "subordinate_clause_per_token_cross_entropy = cross_entropy_per_token(\n",
        "    model_logits, prompt_input_ids, subordinate_clause_tokenized\n",
        ")\n",
        "\n",
        "# Mean cross-entropy of the subordinate clause\n",
        "mean_subordinate_clause_cross_entropy = subordinate_clause_per_token_cross_entropy[\n",
        "    \"cross_entropy\"\n",
        "].mean()\n",
        "\n",
        "# Perplexity of the subordinate clause\n",
        "subordinate_clause_perplexity = np.exp(mean_subordinate_clause_cross_entropy)\n",
        "\n",
        "# Cross-entropy of the subordinating conjunction\n",
        "subordinating_conjunction_cross_entropy = cross_entropy_at(\n",
        "    model_logits, prompt_input_ids, sconj_token_index\n",
        ")\n",
        "\n",
        "# Perplexity of the subordinating conjunction\n",
        "subordinating_conjunction_perplexity = np.exp(subordinating_conjunction_cross_entropy)"
      ],
      "id": "4cb2f59f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print in the order of the sentence structure\n",
        "if independent_clause_index > subordinate_clause_index:\n",
        "    print(\"Structure:\")\n",
        "    print(\"\\t<sentence-with-SCONJ> ::= <subordinate-clause> <independent-clause>\")\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> cross-entropy: {mean_sentence_cross_entropy}\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> perplexity:    {sentence_perplexity}\\n\")\n",
        "    print(f\"\\t<subordinate-clause> cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n",
        "    print(f\"\\t<subordinate-clause> perplexity:     {subordinate_clause_perplexity}\")\n",
        "    print(f\"\\t<SCONJ> cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n",
        "    print(f\"\\t<SCONJ> perplexity:                  {subordinating_conjunction_perplexity}\")\n",
        "    print(f\"\\t<independent-clause> cross-entropy:  {mean_independent_clause_cross_entropy}\")\n",
        "    print(f\"\\t<independent-clause> perplexity:     {independent_clause_perplexity}\")\n",
        "else:\n",
        "    print(\"Structure:\")\n",
        "    print(\"\\t<sentence-with-SCONJ> ::= <independent-clause> <subordinate-clause>\")\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> cross-entropy: {mean_sentence_cross_entropy}\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> perplexity:    {sentence_perplexity}\\n\")\n",
        "    print(f\"\\t<independent-clause> cross-entropy:  {mean_independent_clause_cross_entropy}\")\n",
        "    print(f\"\\t<independent-clause> perplexity:     {independent_clause_perplexity}\")\n",
        "    print(f\"\\t<SCONJ> cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n",
        "    print(f\"\\t<SCONJ> perplexity:                  {subordinating_conjunction_perplexity}\")\n",
        "    print(f\"\\t<subordinate-clause> cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n",
        "    print(f\"\\t<subordinate-clause> perplexity:     {subordinate_clause_perplexity}\")"
      ],
      "id": "aeb475b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### When Context Is Randomly Shuffled\n",
        "\n",
        "Let's shuffle the context and feed it into the model to get the predicted logits:\n"
      ],
      "id": "023aa5a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "last_sconj_index = selected_book[selected_book[\"has_sconj\"]].index[-3]\n",
        "\n",
        "context = selected_book.iloc[max(last_sconj_index - 100, 0) : last_sconj_index][\n",
        "    \"sentence\"\n",
        "].tolist()\n",
        "\n",
        "random.Random(42).shuffle(context)\n",
        "\n",
        "context = \" \".join(context)\n",
        "\n",
        "sentence = selected_book.iloc[last_sconj_index][\"sentence\"]"
      ],
      "id": "22c1fde3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a small peek at the context:\n"
      ],
      "id": "2fc7a9f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print the first 50 characters of the context\n",
        "print(context[:100].strip() + \" ... \", end=\"\")\n",
        "\n",
        "# Print the last 50 characters of the context\n",
        "print(context[-100:].strip(), end=\"\\n\\n\")"
      ],
      "id": "c2b41287",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the sentence with the subordinating conjunction:\n"
      ],
      "id": "ca99d72e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print the sentence\n",
        "print(sentence)"
      ],
      "id": "4ac8a77a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same steps as before:\n"
      ],
      "id": "374ef795"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Tokenize the context (to be used later, not as an input sequence)\n",
        "context_tokenized = tokenizer(context, return_tensors=\"pt\").to(device)\n",
        "context_input_ids = context_tokenized.input_ids\n",
        "\n",
        "# Tokenize the sentence (to be used later, not as an input sequence)\n",
        "sentence_tokenized = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "sentence_input_ids = sentence_tokenized.input_ids\n",
        "\n",
        "# Tokenize the context and the sentence as an input sequence\n",
        "prompt_tokenized = tokenizer(context + sentence, return_tensors=\"pt\").to(device)\n",
        "prompt_input_ids = prompt_tokenized.input_ids\n",
        "\n",
        "# Get the predicted logits for the input sequence\n",
        "model_logits = model(prompt_input_ids).logits"
      ],
      "id": "19cd53cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Get the subordinating conjunction from the book\n",
        "sconj = selected_book.iloc[last_sconj_index][\"sconj\"]\n",
        "\n",
        "# Decode the context as a string, excluding the first token\n",
        "context_decoded = [tokenizer.decode(token) for token in context_input_ids[0]][1:]\n",
        "\n",
        "# Decode the sentence as a string, excluding the first token\n",
        "sentence_decoded = [tokenizer.decode(token) for token in sentence_input_ids[0]][1:]\n",
        "\n",
        "# Index of the subordinating conjunction token in the input sequence\n",
        "sconj_token_index = len(context_decoded) + sentence_decoded.index(sconj)\n",
        "\n",
        "# Index of the subordinating conjunction in the sentence (not the input sequence, but from the book)\n",
        "sconj_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(sconj)\n",
        "\n",
        "# Figure out which type of clause comes first\n",
        "if sconj not in sentence[:sconj_index]:\n",
        "    independent_clause = sentence[:sconj_index]\n",
        "    subordinate_clause = sentence[sconj_index:]\n",
        "else:\n",
        "    independent_clause = sentence[sconj_index:]\n",
        "    subordinate_clause = sentence[:sconj_index]\n",
        "\n",
        "# Index of the independent clause in the sentence (not the input sequence, but from the book)\n",
        "independent_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n",
        "    independent_clause\n",
        ")\n",
        "\n",
        "# Index of the subordinate clause in the sentence (not the input sequence, but from the book)\n",
        "subordinate_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n",
        "    subordinate_clause\n",
        ")\n",
        "\n",
        "# Tokenize the independent clause\n",
        "independent_clause_tokenized = tokenizer(\n",
        "    independent_clause, return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "# Tokenize the subordinate clause\n",
        "subordinate_clause_tokenized = tokenizer(\n",
        "    subordinate_clause, return_tensors=\"pt\"\n",
        ").to(device)"
      ],
      "id": "016acf21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given that we have fed in a randomly shuffled context, let's take a look at the top k probability spectrum at the subordinating conjunction:\n"
      ],
      "id": "5f37043a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prob_spectrum = probability_spectrum_at(\n",
        "    model_logits, prompt_input_ids, sconj_token_index\n",
        ")\n",
        "\n",
        "prob_spectrum"
      ],
      "id": "ce920b15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also look at the cross-entropy and perplexity of the sentence with the subordinating conjunction:\n"
      ],
      "id": "635b9789"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\n",
        "sentence_per_token_cross_entropy = cross_entropy_per_token(\n",
        "    model_logits, prompt_input_ids, sentence_tokenized\n",
        ")\n",
        "\n",
        "# Mean cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\n",
        "mean_sentence_cross_entropy = sentence_per_token_cross_entropy[\"cross_entropy\"].mean()\n",
        "\n",
        "# Perplexity of the sentence with the subordinating conjunction (entire input sequence)\n",
        "sentence_perplexity = np.exp(mean_sentence_cross_entropy)\n",
        "\n",
        "# Cross-entropy of the independent clause\n",
        "independent_clause_per_token_cross_entropy = cross_entropy_per_token(\n",
        "    model_logits, prompt_input_ids, independent_clause_tokenized\n",
        ")\n",
        "\n",
        "# Mean cross-entropy of the independent clause\n",
        "mean_independent_clause_cross_entropy = independent_clause_per_token_cross_entropy[\n",
        "    \"cross_entropy\"\n",
        "].mean()\n",
        "\n",
        "# Perplexity of the independent clause\n",
        "independent_clause_perplexity = np.exp(mean_independent_clause_cross_entropy)\n",
        "\n",
        "# Cross-entropy of the subordinate clause\n",
        "subordinate_clause_per_token_cross_entropy = cross_entropy_per_token(\n",
        "    model_logits, prompt_input_ids, subordinate_clause_tokenized\n",
        ")\n",
        "\n",
        "# Mean cross-entropy of the subordinate clause\n",
        "mean_subordinate_clause_cross_entropy = subordinate_clause_per_token_cross_entropy[\n",
        "    \"cross_entropy\"\n",
        "].mean()\n",
        "\n",
        "# Perplexity of the subordinate clause\n",
        "subordinate_clause_perplexity = np.exp(mean_subordinate_clause_cross_entropy)\n",
        "\n",
        "# Cross-entropy of the subordinating conjunction\n",
        "subordinating_conjunction_cross_entropy = cross_entropy_at(\n",
        "    model_logits, prompt_input_ids, sconj_token_index\n",
        ")\n",
        "\n",
        "# Perplexity of the subordinating conjunction\n",
        "subordinating_conjunction_perplexity = np.exp(subordinating_conjunction_cross_entropy)"
      ],
      "id": "4e72c681",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Print in the order of the sentence structure\n",
        "if independent_clause_index > subordinate_clause_index:\n",
        "    print(\"Structure:\")\n",
        "    print(\"\\t<sentence-with-SCONJ> ::= <subordinate-clause> <independent-clause>\")\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> cross-entropy: {mean_sentence_cross_entropy}\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> perplexity:    {sentence_perplexity}\\n\")\n",
        "    print(f\"\\t<subordinate-clause> cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n",
        "    print(f\"\\t<subordinate-clause> perplexity:     {subordinate_clause_perplexity}\")\n",
        "    print(f\"\\t<SCONJ> cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n",
        "    print(f\"\\t<SCONJ> perplexity:                  {subordinating_conjunction_perplexity}\")\n",
        "    print(f\"\\t<independent-clause> cross-entropy:  {mean_independent_clause_cross_entropy}\")\n",
        "    print(f\"\\t<independent-clause> perplexity:     {independent_clause_perplexity}\")\n",
        "else:\n",
        "    print(\"Structure:\")\n",
        "    print(\"\\t<sentence-with-SCONJ> ::= <independent-clause> <subordinate-clause>\")\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> cross-entropy: {mean_sentence_cross_entropy}\")\n",
        "    print(f\"\\t<sentence-with-SCONJ> perplexity:    {sentence_perplexity}\\n\")\n",
        "    print(f\"\\t<independent-clause> cross-entropy:  {mean_independent_clause_cross_entropy}\")\n",
        "    print(f\"\\t<independent-clause> perplexity:     {independent_clause_perplexity}\")\n",
        "    print(f\"\\t<SCONJ> cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n",
        "    print(f\"\\t<SCONJ> perplexity:                  {subordinating_conjunction_perplexity}\")\n",
        "    print(f\"\\t<subordinate-clause> cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n",
        "    print(f\"\\t<subordinate-clause> perplexity:     {subordinate_clause_perplexity}\")"
      ],
      "id": "35d4c543",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and Conclusion\n",
        "\n",
        "Our analysis section demonstrates that the cross-entropy and perplexity of the sentence with the subordinating conjunction change based on the context provided to the model. Furthermore, we observed that the probability of the subordinating conjunction is also affected by the context. This suggests that the context provided to the model is important for predicting subordinating conjunctions. In other words, the context provided to the model can influence the likelihood of the model predicting subordinating conjunctions. Moreover, we have also observed that, despite the change in the context, the cross-entropy and perplexity around the subordinate clause did not change as much as around the independent clause. Although this warrants more thorough investigation, it suggests that there is a certain kind of subordinating conjunction that appears to be more useful, even to the LLM (as it was still likely to construct the same subordinate clause even with the contextual change).\n",
        "\n",
        "## Limitations\n",
        "\n",
        "Our work is not without limitations. Firstly, we have only analyzed the LLM with one book, which is not representative of different kinds of writing contexts. Furthermore, our approach is currently only able to parse subordinate clauses that position the subordinating conjunction in the middle of the sentence. There are certain edge cases related to the positioning of the subordinating conjunctions that we have not considered.\n",
        "\n",
        "## Future Work\n",
        "\n",
        "A natural extension of this work is to evaluate the LLM's likelihood of predicting subordinating conjunctions with a more diverse and representative sample of data. Furthermore, we can also evaluate the LLM's likelihood of predicting other kinds of conjunctions, such as coordinating conjunctions. Moreover, we can also evaluate the LLM's likelihood of predicting subordinating conjunctions in different kinds of writing contexts, such as academic writing instead of books. Another way to extend this work is to evaluate the relationship between the LLM's hyperparameters and its likelihood of predicting subordinating conjunctions.\n",
        "\n",
        "## Appendix\n",
        "\n",
        "### AutoAWQ Quantization\n",
        "\n",
        "In this section, we have documented our approach to quantizing the [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model using [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) into 4-bit precision. This reduces the amount of computational resources required to run inference on the model while still maintaining a high level of accuracy.\n",
        "\n",
        "```{.python}\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer, AwqConfig\n",
        "```\n",
        "\n",
        "```{.python}\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "quantized_model_path = \"Llama-2-7b-chat-hf-awq\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoAWQForCausalLM.from_pretrained(model_name, **{\"low_cpu_mem_usage\": True})\n",
        "```\n",
        "\n",
        "```{.python}\n",
        "# Setup AutoAWQ quantization configuration\n",
        "quant_config = {\n",
        "    \"zero_point\": True,\n",
        "    \"q_group_size\": 128,\n",
        "    \"w_bit\": 4,\n",
        "    \"version\": \"GEMM\",\n",
        "}\n",
        "\n",
        "# Quantize the model\n",
        "model.quantize(tokenizer, quant_config=quant_config)\n",
        "```\n",
        "\n",
        "```{.python}\n",
        "# Setup Transformer compatible quantization configuration\n",
        "quantization_config = AwqConfig(\n",
        "    bits=quant_config[\"w_bit\"],\n",
        "    group_size=quant_config[\"q_group_size\"],\n",
        "    zero_point=quant_config[\"zero_point\"],\n",
        "    version=quant_config[\"version\"].lower(),\n",
        ").to_dict()\n",
        "\n",
        "# Pass the new quantization configuration to the model\n",
        "model.model.config.quantization_config = quantization_config\n",
        "\n",
        "# Save the quantized model weights\n",
        "tokenizer.save_pretrained(quantized_model_path)\n",
        "model.save_quantized(quantized_model_path)\n",
        "```\n",
        "\n",
        "To promote reproducibility of this work, we have uploaded our quantized model to Hugging Face repositories. You can access our quantized model here: [CalvinU/Llama-2-7b-chat-hf-awq](https://huggingface.co/CalvinU/Llama-2-7b-chat-hf-awq).\n",
        "\n",
        "### Scalable Data Collection\n",
        "\n",
        "In the EDA, we have only looked at one book. However, in a language modeling task, we would likely need a _sample_ of data that is diverse and representative of different kinds of writing. In this section, we have documented a scalable approach to collecting and processing data from [Project Gutenberg](https://www.gutenberg.org/).\n",
        "\n",
        "```{.python}\n",
        "import pandas as pd\n",
        "import requests\n",
        "import random\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "```\n",
        "\n",
        "Here are the functions we have used. All of them were already defined in the EDA section, except for `download_books`, which is a wrapper function for `download_book` that downloads multiple books instead of just one:\n",
        "\n",
        "<details>\n",
        "<summary>Code</summary>\n",
        "```{.python}\n",
        "def download_book(book_id: int) -> tuple[str, str]:\n",
        "    \"\"\"Download a book from Project Gutenberg\n",
        "\n",
        "    Arg:\n",
        "        book_id: The Project Gutenberg ID of the book to download\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the book title and the book text\n",
        "    \"\"\"\n",
        "\n",
        "    gutendex_url = f\"https://gutendex.com/books/{book_id}/\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(gutendex_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        book_language = data[\"languages\"]\n",
        "\n",
        "        # Only download books in English\n",
        "        if \"en\" in book_language:\n",
        "            book_title = data[\"title\"]\n",
        "\n",
        "            # Only download books in plain text\n",
        "            mime_types = [\"text/plain\", \"text/plain; charset=us-ascii\"]\n",
        "\n",
        "            for mime_type in mime_types:\n",
        "                if mime_type in data[\"formats\"]:\n",
        "                    book_url = data[\"formats\"][mime_type]\n",
        "                    break\n",
        "\n",
        "            if book_url is None:\n",
        "                raise Exception(\"The book is not available in plain text.\")\n",
        "\n",
        "            response = requests.get(book_url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            return book_title, response.text\n",
        "        else:\n",
        "            raise Exception(\"The book is not in English.\")\n",
        "    except requests.exceptions.HTTPError as err:\n",
        "        raise Exception(err)\n",
        "\n",
        "\n",
        "def download_books(n: int) -> list[tuple[int, str, str]]:\n",
        "    \"\"\"Download n books from Project Gutenberg\n",
        "\n",
        "    Arg:\n",
        "        n: The number of books to download\n",
        "\n",
        "    Returns:\n",
        "        A list of downloaded books\n",
        "    \"\"\"\n",
        "\n",
        "    max_book_count = requests.get(\"https://gutendex.com/books/\").json()[\"count\"]\n",
        "\n",
        "    books = []\n",
        "\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        book_id = random.randint(1, max_book_count)\n",
        "\n",
        "        try:\n",
        "            book_title, book_text = download_book(book_id)\n",
        "            books.append((book_id, book_title, book_text))\n",
        "            i += 1\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return books\n",
        "\n",
        "\n",
        "def sanitize_text(text: str) -> str:\n",
        "    \"\"\"Remove extra information from the text\n",
        "\n",
        "    Arg:\n",
        "        text: The text to sanitize\n",
        "\n",
        "    Returns:\n",
        "        The sanitized text\n",
        "    \"\"\"\n",
        "\n",
        "    start_marker = \"***\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "    # Index of the second occurrence of the start marker\n",
        "    start_index = text.find(start_marker, text.find(start_marker) + 1)\n",
        "\n",
        "    # Index of the first occurrence of the end marker\n",
        "    end_index = text.find(end_marker)\n",
        "\n",
        "    # Remove the extra information based on the marker indices\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        text = text[start_index + len(start_marker) : end_index].strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def sentence_spliter(text: str) -> list[str]:\n",
        "    \"\"\"Split the text into sentences\n",
        "\n",
        "    Arg:\n",
        "        text: The text to split\n",
        "\n",
        "    Returns:\n",
        "        A list of sentences\n",
        "    \"\"\"\n",
        "\n",
        "    nlp.max_length = len(text)\n",
        "\n",
        "    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n",
        "\n",
        "    # Remove line breaks and split the text into sentences\n",
        "    doc = nlp.pipe([text.replace(\"\\r\\n\", \" \")], disable=pipe_disable)\n",
        "\n",
        "    # Return a list of sentences without leading and trailing whitespace\n",
        "    return [sent.text.strip() for doc in doc for sent in doc.sents]\n",
        "```\n",
        "</details>\n",
        "\n",
        "Download 10 random books from [Project Gutenberg](https://www.gutenberg.org/):\n",
        "\n",
        "```{.python}\n",
        "n_books = 10\n",
        "\n",
        "books10 = pd.DataFrame(\n",
        "    download_books(n_books), \n",
        "    columns=[\"book_id\", \"title\", \"text\"]\n",
        ")\n",
        "\n",
        "assert len(books10) == n_books\n",
        "```\n",
        "\n",
        "Clean the texts:\n",
        "\n",
        "```{.python}\n",
        "books10[\"clean_text\"] = books10[\"text\"].apply(sanitize_text)\n",
        "```\n",
        "\n",
        "Split the texts into sentences:\n",
        "\n",
        "```{.python}\n",
        "books10_sentences = []\n",
        "\n",
        "# For each book, split the text into sentences\n",
        "for i in range(0, len(books10)):\n",
        "    books10_sentences.append(\n",
        "        (\n",
        "            books10[\"book_id\"].iloc[i],\n",
        "            books10[\"title\"].iloc[i],\n",
        "            sentence_spliter(books10[\"clean_text\"].iloc[i]),\n",
        "        )\n",
        "    )\n",
        "```\n",
        "\n",
        "Create a new DataFrame with the sentences:\n",
        "\n",
        "```{.python}\n",
        "# For each sentences in each id, create a new row\n",
        "books10_sentences = [\n",
        "    (id, title, sent) for id, title, sents in books10_sentences for sent in sents\n",
        "]\n",
        "\n",
        "books10_sentences = pd.DataFrame(\n",
        "    books10_sentences, columns=[\"book_id\", \"title\", \"sentence\"]\n",
        ")\n",
        "```\n",
        "\n",
        "To promote reproducibility of this work, we have saved the data we have collected and processed using this approach as a [parquet](https://parquet.apache.org/) file. You can view and access our dataset here: [CalvinU/project-gutenberg](https://huggingface.co/datasets/CalvinU/project-gutenberg).\n",
        "\n",
        "[^1]: https://www.merriam-webster.com/dictionary/inspire\n",
        "\n",
        "[^2]: Chosen for its simplicity, however, other quantization methods, such as [llama.cpp](https://github.com/ggerganov/llama.cpp), will also likely work."
      ],
      "id": "691d26cd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}