[
  {
    "objectID": "portfolio/index.html",
    "href": "portfolio/index.html",
    "title": "Jiho Kim’s Portfolio",
    "section": "",
    "text": "Authors: Jiho Kim, Ray C. Flanagan, Noelle E. Haviland, ZeAi Sun, Souad N. Yakubu, Edom A. Maru, and Kenneth C. Arnold\nPublished: March 2, 2024\n\n\n\n\n\n\nFigure 1: UI of the writing assistant implemented as Microsoft Word add-in.\n\n\n\nThe current interaction paradigm of writing with AI is dominated by humans delegating some or all creative control to AI, instead of maintaining that control themselves. We observed that this was because the current UI design of chatbots was not designed to allow writers to engage in meaningful creative processes and instead encouraged overreliance on automatic adjustments. We identified an opportunity to support writers during revision–specifically, the crucial process of transforming writer-focused drafts into audience-focused prose. In our collaborative project, I led the development of an interactive system where writers could choose and modify predefined prompts to access targeted AI insights (including descriptive summaries, reflective questions, and suggestions), which we called AI views, specific to their selected paragraphs (see Figure 1). These novel UI affordances preserved writers’ creative agency and ownership while providing revision support.\n\n\n\nDownload PDF file.\n\n\nFigure 2: Published workshop paper. Please visit this link to see it in a new tab.\n\n\n\n\n\n\n\n\n\nFigure 3: Recorded workshop presentation.\n\n\n\nOur results, which I qualitatively coded from user studies, showed that participants using our system were better able to consider their audience’s needs, enhance clarity, and uncover previously overlooked ideas during the revision process. Our findings motivate a design principle: rather than defaulting to automated solutions, these human-AI co-creative systems should augment human creative capabilities by encouraging active participation in the creative process. I published (see Figure 2) and presented (see Figure 3) our work as the first author at the HAI-GEN workshop at ACM IUI 2024.\n\n\n\nAuthors: Jiho Kim, Jooha Yoo, Jaden L. Brookens\nPublished: October 15, 2024\nAt Calvin University, where I am currently finishing my senior year, we have an annual event called “Cokes & Clubs” where students and student club organizers meet to learn about new student clubs and recruit new members. However, this event only happens once a year, and through needfinding, we realized that many students want opportunities to discover and find new student clubs to participate in throughout the school year. Furthermore, club organizers wanted a platform where they could easily advertise and recruit new members to their clubs in a sustainable manner (e.g., without needing to print hundreds of flyers for their events or clubs).\nSo, in a collaborative effort (as an HCI class project), I led the design of a centralized platform for students and club leaders to manage and promote club events, enable event discovery, and increase participation and engagement across campus throughout the year.\n\n\n\n\n\n\nFigure 4: First iteration of our prototype (parallel prototyping).\n\n\n\n\n\n\n\n\n\nFigure 5: Second iteration of our prototype before user studies.\n\n\n\n\n\n\n\n\n\nFigure 6: Final iteration of our prototype after user studies.\n\n\n\nWe started by prototyping (in parallel) what the users might want (see Figure 4) and sketched a possible UI using Canva (see Figure 5), which we later simulated using Figma. We ran user studies to check our assumptions, through which we discovered many incorrect assumptions but also identified correct ones that we were able to reinforce in our final iteration (see Figure 6).\n\n\n\n\n\n\nFigure 7: UI walkthrough of our final prototype.\n\n\n\nAlthough we did not actually implement our design as a functioning app, we demonstrated a walkthrough of our UI in the final class presentation (see Figure 7).\nThis class project has deepened my understanding of HCI beyond my previous research experience (i.e., “Towards Full Authorship with AI: Supporting Revision with AI-Generated Views”), particularly emphasizing the value of thorough needfinding through contextual inquiry and storyboarding. It has transformed my approach from researcher-centered to user-centered design, while teaching me to thoughtfully synthesize user feedback to uncover fundamental design challenges rather than simply implementing every suggestion."
  },
  {
    "objectID": "portfolio/index.html#towards-full-authorship-with-ai-supporting-revision-with-ai-generated-views",
    "href": "portfolio/index.html#towards-full-authorship-with-ai-supporting-revision-with-ai-generated-views",
    "title": "Jiho Kim’s Portfolio",
    "section": "",
    "text": "Authors: Jiho Kim, Ray C. Flanagan, Noelle E. Haviland, ZeAi Sun, Souad N. Yakubu, Edom A. Maru, and Kenneth C. Arnold\nPublished: March 2, 2024\n\n\n\n\n\n\nFigure 1: UI of the writing assistant implemented as Microsoft Word add-in.\n\n\n\nThe current interaction paradigm of writing with AI is dominated by humans delegating some or all creative control to AI, instead of maintaining that control themselves. We observed that this was because the current UI design of chatbots was not designed to allow writers to engage in meaningful creative processes and instead encouraged overreliance on automatic adjustments. We identified an opportunity to support writers during revision–specifically, the crucial process of transforming writer-focused drafts into audience-focused prose. In our collaborative project, I led the development of an interactive system where writers could choose and modify predefined prompts to access targeted AI insights (including descriptive summaries, reflective questions, and suggestions), which we called AI views, specific to their selected paragraphs (see Figure 1). These novel UI affordances preserved writers’ creative agency and ownership while providing revision support.\n\n\n\nDownload PDF file.\n\n\nFigure 2: Published workshop paper. Please visit this link to see it in a new tab.\n\n\n\n\n\n\n\n\n\nFigure 3: Recorded workshop presentation.\n\n\n\nOur results, which I qualitatively coded from user studies, showed that participants using our system were better able to consider their audience’s needs, enhance clarity, and uncover previously overlooked ideas during the revision process. Our findings motivate a design principle: rather than defaulting to automated solutions, these human-AI co-creative systems should augment human creative capabilities by encouraging active participation in the creative process. I published (see Figure 2) and presented (see Figure 3) our work as the first author at the HAI-GEN workshop at ACM IUI 2024."
  },
  {
    "objectID": "portfolio/index.html#calvent-year-long-cokes-clubs",
    "href": "portfolio/index.html#calvent-year-long-cokes-clubs",
    "title": "Jiho Kim’s Portfolio",
    "section": "",
    "text": "Authors: Jiho Kim, Jooha Yoo, Jaden L. Brookens\nPublished: October 15, 2024\nAt Calvin University, where I am currently finishing my senior year, we have an annual event called “Cokes & Clubs” where students and student club organizers meet to learn about new student clubs and recruit new members. However, this event only happens once a year, and through needfinding, we realized that many students want opportunities to discover and find new student clubs to participate in throughout the school year. Furthermore, club organizers wanted a platform where they could easily advertise and recruit new members to their clubs in a sustainable manner (e.g., without needing to print hundreds of flyers for their events or clubs).\nSo, in a collaborative effort (as an HCI class project), I led the design of a centralized platform for students and club leaders to manage and promote club events, enable event discovery, and increase participation and engagement across campus throughout the year.\n\n\n\n\n\n\nFigure 4: First iteration of our prototype (parallel prototyping).\n\n\n\n\n\n\n\n\n\nFigure 5: Second iteration of our prototype before user studies.\n\n\n\n\n\n\n\n\n\nFigure 6: Final iteration of our prototype after user studies.\n\n\n\nWe started by prototyping (in parallel) what the users might want (see Figure 4) and sketched a possible UI using Canva (see Figure 5), which we later simulated using Figma. We ran user studies to check our assumptions, through which we discovered many incorrect assumptions but also identified correct ones that we were able to reinforce in our final iteration (see Figure 6).\n\n\n\n\n\n\nFigure 7: UI walkthrough of our final prototype.\n\n\n\nAlthough we did not actually implement our design as a functioning app, we demonstrated a walkthrough of our UI in the final class presentation (see Figure 7).\nThis class project has deepened my understanding of HCI beyond my previous research experience (i.e., “Towards Full Authorship with AI: Supporting Revision with AI-Generated Views”), particularly emphasizing the value of thorough needfinding through contextual inquiry and storyboarding. It has transformed my approach from researcher-centered to user-centered design, while teaching me to thoughtfully synthesize user feedback to uncover fundamental design challenges rather than simply implementing every suggestion."
  },
  {
    "objectID": "portfolio/index.html#inspirative-text-prediction",
    "href": "portfolio/index.html#inspirative-text-prediction",
    "title": "Jiho Kim’s Portfolio",
    "section": "Inspirative Text Prediction",
    "text": "Inspirative Text Prediction\nAuthor: Jiho Kim\nPublished: December 12, 2023\nThis work (see Figure 8) is a technical blog post exploring an inspirative text prediction system aimed at augmenting human creativity in using predictive text by encouraging reflective and original thought. Unlike traditional predictive models that suggest the most probable next words, this system leverages linguistic constructs, such as subordinating conjunctions (e.g., “because”), to prompt deeper engagement with the text. Using datasets from Project Gutenberg, the project analyzed sentence structures and linguistic patterns with tools like spaCy and evaluated the latest Llama model (at the time of the publication). The findings highlighted subordinating conjunctions as effective prompts for provocative thinking. The blog post includes a scalable pipeline for data collection and processing, visualizations of linguistic features, and recommendations for extending the approach to diverse datasets and linguistic phenomena.\n\n\n\n\n\n\nFigure 8: Blog post on inspirative text prediction system. Please visit this link to see it in a new tab."
  },
  {
    "objectID": "pubs/index.html",
    "href": "pubs/index.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nTowards Full Authorship with AI: Supporting Revision with AI-Generated Views\n\n\n\n\n\n\nWorkshop paper\n\n\n\nAccepted to 5th Workshop on Human-AI Co-Creation with Generative Models (HAI-GEN) at ACM IUI 2024\n\n\n\n\n\nMar 2, 2024\n\n\nJiho Kim, Ray C. Flanagan, Noelle E. Haviland, ZeAi Sun, Souad N. Yakubu, Edom A. Maru, Kenneth C. Arnold\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiho Kim",
    "section": "",
    "text": "Hi there! I am a senior undergraduate student studying Computer Science at Calvin University. I am advised by Ken Arnold.\nMy research interest is in human-AI interaction. I design, build, and study intelligent and interactive systems that think with people rather than for them, augmenting human intelligence and creativity with AI."
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Jiho Kim",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthors\n\n\n\n\n\n\n3/2/24\n\n\nTowards Full Authorship with AI: Supporting Revision with AI-Generated Views\n\n\nJiho Kim, Ray C. Flanagan, Noelle E. Haviland, ZeAi Sun, Souad N. Yakubu, Edom A. Maru, Kenneth C. Arnold\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Jiho Kim",
    "section": "Updates",
    "text": "Updates\n\n\nNovember 2, 2024: I was invited to give a talk about my undergraduate research at WMRUGS ’24 in Grand Rapids, MI.\nSeptember 6, 2024: I started working as a research intern with Xiang “Anthony” Chen at UCLA HCI Research.\nJune 20, 2024: I started working as a summer research intern with Juho Kim at KIXLAB.\nMarch 18, 2024: I presented our workshop paper from my summer research at the HAI-GEN workshop at ACM IUI ’24 in Greenville, SC.\nFebruary 9, 2024: I was awarded a $1,290 travel grant from the NSF to attend ACM IUI ’24 as a student volunteer.\nNovember 4, 2023: I presented our poster from my summer research at WMRUGS ’23 at the Van Andel Institute in Grand Rapids, MI.\nJune 1, 2023: I started working as an undergraduate research assistant with Ken Arnold at Calvin University."
  },
  {
    "objectID": "proj/statistical-inference/index.html",
    "href": "proj/statistical-inference/index.html",
    "title": "Statistical Inference in Nutshell",
    "section": "",
    "text": "If the population follows a normal distribution or if the sample size n is sufficiently large so that n \\hat{p} \\geq 10 and n (1 - \\hat{p}) \\geq 10 for the sample proportion \\hat{p}, then a confidence interval for the population proportion p can be calculated as: \\hat{p} \\pm z^* \\sqrt{\\dfrac{\\hat{p} (1 - \\hat{p})}{n}} where z^* is chosen such that P(-z^* &lt; Z &lt; z^*) = 1 - \\alpha for a given significance level \\alpha.\n\n\nSuppose we are given a sample size of n = 400, a sample proportion of \\hat{p} = 0.84, and a significance level of \\alpha = 0.05 (or a 1 - \\alpha = 0.95 confidence level).\nWithout knowing whether the population follows a normal distribution, we verify that both n \\hat{p} = (400)(0.84) = 336 \\geq 10 and n (1 - \\hat{p}) = (400)(0.16) = 64 \\geq 10 are satisfied. Therefore, we can proceed with calculating the confidence interval:\n\nn &lt;- 400\np_hat &lt;- 0.84\nalpha &lt;- 0.05\n\n1z_star &lt;- qnorm(1 - (alpha / 2))\nstandard_error &lt;- sqrt((p_hat * (1 - p_hat)) / n)\n2interval &lt;- p_hat + c(-1, 1) * z_star * standard_error\n\ncat(\"(\", round(interval[1], 4), \", \", round(interval[2], 4), \")\", sep = \"\")\n\n\n1\n\nDivide alpha by 2 to determine the area for one tail, then subtract it from 1 to obtain the area up to the right tail. Then find the z-score that corresponds to this area using the qnorm function.\n\n2\n\nMultiplying the critical value by c(-1, 1) is just a lazy yet convenient way to calculate the lower and upper bounds of the confidence interval simultaneously.\n\n\n\n\n(0.8041, 0.8759)\n\n\n\n\n\n\n\n\nTo determine the sample size n required to estimate the population proportion p with a desired margin of error ME and a significance level \\alpha, we can use the following formula: n = \\lceil\\bigg(\\dfrac{z^*}{ME}\\bigg)^2 \\tilde{p} (1 - \\tilde{p})\\rceil where z^* is chosen such that P(-z^* &lt; Z &lt; z^*) = 1 - \\alpha for a given significance level \\alpha and \\tilde{p} is an estimate of the population proportion p (use \\tilde{p} = 0.5 if no estimate is available).\n\n\n\nTo test the null hypothesis H_0: p = p_0 against the alternative hypothesis H_1: p \\neq p_0 (or a one-tailed alternative), we use the following test statistic: z = \\dfrac{\\hat{p} - p_0}{\\sqrt{\\dfrac{p_0 (1 - p_0)}{n}}} where n is the sample size and \\hat{p} is the sample proportion, provided that the population follows a normal distribution or that the sample size n is sufficiently large so that n p_0 \\geq 10 and n (1 - p_0) \\geq 10.\nThere are two approaches to hypothesis testing: critical value approach and p-value approach.\n\n\nSuppose we test the null hypothesis H_0: p = 0.8 against the alternative hypothesis H_1: p \\neq 0.8 with a significance level of \\alpha = 0.05. Then we can calculate the critical value as follows:\n\nalpha &lt;- 0.05\nz_star &lt;- qnorm(1 - (alpha / 2))\nz_star\n\n[1] 1.959964\n\n\nWe collect a sample of size n = 400 with a sample proportion of \\hat{p} = 0.84. Without knowing whether the population follows a normal distribution, we verify that both np_0 = (400)(0.8) = 320 \\geq 10 and n(1 - p_0) = (400)(0.2) = 80 \\geq 10 are satisfied. Therefore, we can proceed with calculating the test statistic:\n\nn &lt;- 400\np_hat &lt;- 0.84\np_0 &lt;- 0.8\n\nz &lt;- (p_hat - p_0) / sqrt((p_0 * (1 - p_0)) / n)\nz\n\n[1] 2\n\n\nIf the absolute value of the test statistic |z| is greater than the critical value z^*, we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\n\nabs(z) &gt; z_star\n\n[1] TRUE\n\n\nTherefore, we reject the null hypothesis.\n\n\n\nSuppose we test the null hypothesis H_0: p = 0.8 against the alternative hypothesis H_1: p \\neq 0.8 with a significance level of \\alpha = 0.05.\nWe collect a sample of size n = 400 with a sample proportion of \\hat{p} = 0.84. Without knowing whether the population follows a normal distribution, we verify that both np_0 = (400)(0.8) = 320 \\geq 10 and n(1 - p_0) = (400)(0.2) = 80 \\geq 10 are satisfied. Therefore, we can proceed with calculating the test statistic:\n\nn &lt;- 400\np_hat &lt;- 0.84\np_0 &lt;- 0.8\n\nz &lt;- (p_hat - p_0) / sqrt((p_0 * (1 - p_0)) / n)\nz\n\n[1] 2\n\n\nThe p-value is the probability of observing a test statistic as extreme as the one calculated from the collected sample, assuming that the null hypothesis is true. The p-value can be calculated as follows:\n\np_value &lt;- 2 * pnorm(-abs(z))\np_value\n\n[1] 0.04550026\n\n\nIf the p-value is less than \\alpha, we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\n\nalpha &lt;- 0.05\np_value &lt; alpha\n\n[1] TRUE\n\n\nTherefore, we reject the null hypothesis."
  },
  {
    "objectID": "proj/statistical-inference/index.html#single-proportion",
    "href": "proj/statistical-inference/index.html#single-proportion",
    "title": "Statistical Inference in Nutshell",
    "section": "",
    "text": "If the population follows a normal distribution or if the sample size n is sufficiently large so that n \\hat{p} \\geq 10 and n (1 - \\hat{p}) \\geq 10 for the sample proportion \\hat{p}, then a confidence interval for the population proportion p can be calculated as: \\hat{p} \\pm z^* \\sqrt{\\dfrac{\\hat{p} (1 - \\hat{p})}{n}} where z^* is chosen such that P(-z^* &lt; Z &lt; z^*) = 1 - \\alpha for a given significance level \\alpha.\n\n\nSuppose we are given a sample size of n = 400, a sample proportion of \\hat{p} = 0.84, and a significance level of \\alpha = 0.05 (or a 1 - \\alpha = 0.95 confidence level).\nWithout knowing whether the population follows a normal distribution, we verify that both n \\hat{p} = (400)(0.84) = 336 \\geq 10 and n (1 - \\hat{p}) = (400)(0.16) = 64 \\geq 10 are satisfied. Therefore, we can proceed with calculating the confidence interval:\n\nn &lt;- 400\np_hat &lt;- 0.84\nalpha &lt;- 0.05\n\n1z_star &lt;- qnorm(1 - (alpha / 2))\nstandard_error &lt;- sqrt((p_hat * (1 - p_hat)) / n)\n2interval &lt;- p_hat + c(-1, 1) * z_star * standard_error\n\ncat(\"(\", round(interval[1], 4), \", \", round(interval[2], 4), \")\", sep = \"\")\n\n\n1\n\nDivide alpha by 2 to determine the area for one tail, then subtract it from 1 to obtain the area up to the right tail. Then find the z-score that corresponds to this area using the qnorm function.\n\n2\n\nMultiplying the critical value by c(-1, 1) is just a lazy yet convenient way to calculate the lower and upper bounds of the confidence interval simultaneously.\n\n\n\n\n(0.8041, 0.8759)\n\n\n\n\n\n\n\n\nTo determine the sample size n required to estimate the population proportion p with a desired margin of error ME and a significance level \\alpha, we can use the following formula: n = \\lceil\\bigg(\\dfrac{z^*}{ME}\\bigg)^2 \\tilde{p} (1 - \\tilde{p})\\rceil where z^* is chosen such that P(-z^* &lt; Z &lt; z^*) = 1 - \\alpha for a given significance level \\alpha and \\tilde{p} is an estimate of the population proportion p (use \\tilde{p} = 0.5 if no estimate is available).\n\n\n\nTo test the null hypothesis H_0: p = p_0 against the alternative hypothesis H_1: p \\neq p_0 (or a one-tailed alternative), we use the following test statistic: z = \\dfrac{\\hat{p} - p_0}{\\sqrt{\\dfrac{p_0 (1 - p_0)}{n}}} where n is the sample size and \\hat{p} is the sample proportion, provided that the population follows a normal distribution or that the sample size n is sufficiently large so that n p_0 \\geq 10 and n (1 - p_0) \\geq 10.\nThere are two approaches to hypothesis testing: critical value approach and p-value approach.\n\n\nSuppose we test the null hypothesis H_0: p = 0.8 against the alternative hypothesis H_1: p \\neq 0.8 with a significance level of \\alpha = 0.05. Then we can calculate the critical value as follows:\n\nalpha &lt;- 0.05\nz_star &lt;- qnorm(1 - (alpha / 2))\nz_star\n\n[1] 1.959964\n\n\nWe collect a sample of size n = 400 with a sample proportion of \\hat{p} = 0.84. Without knowing whether the population follows a normal distribution, we verify that both np_0 = (400)(0.8) = 320 \\geq 10 and n(1 - p_0) = (400)(0.2) = 80 \\geq 10 are satisfied. Therefore, we can proceed with calculating the test statistic:\n\nn &lt;- 400\np_hat &lt;- 0.84\np_0 &lt;- 0.8\n\nz &lt;- (p_hat - p_0) / sqrt((p_0 * (1 - p_0)) / n)\nz\n\n[1] 2\n\n\nIf the absolute value of the test statistic |z| is greater than the critical value z^*, we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\n\nabs(z) &gt; z_star\n\n[1] TRUE\n\n\nTherefore, we reject the null hypothesis.\n\n\n\nSuppose we test the null hypothesis H_0: p = 0.8 against the alternative hypothesis H_1: p \\neq 0.8 with a significance level of \\alpha = 0.05.\nWe collect a sample of size n = 400 with a sample proportion of \\hat{p} = 0.84. Without knowing whether the population follows a normal distribution, we verify that both np_0 = (400)(0.8) = 320 \\geq 10 and n(1 - p_0) = (400)(0.2) = 80 \\geq 10 are satisfied. Therefore, we can proceed with calculating the test statistic:\n\nn &lt;- 400\np_hat &lt;- 0.84\np_0 &lt;- 0.8\n\nz &lt;- (p_hat - p_0) / sqrt((p_0 * (1 - p_0)) / n)\nz\n\n[1] 2\n\n\nThe p-value is the probability of observing a test statistic as extreme as the one calculated from the collected sample, assuming that the null hypothesis is true. The p-value can be calculated as follows:\n\np_value &lt;- 2 * pnorm(-abs(z))\np_value\n\n[1] 0.04550026\n\n\nIf the p-value is less than \\alpha, we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\n\nalpha &lt;- 0.05\np_value &lt; alpha\n\n[1] TRUE\n\n\nTherefore, we reject the null hypothesis."
  },
  {
    "objectID": "proj/statistical-inference/index.html#difference-in-proportions",
    "href": "proj/statistical-inference/index.html#difference-in-proportions",
    "title": "Statistical Inference in Nutshell",
    "section": "Difference in Proportions",
    "text": "Difference in Proportions\n\nConfidence Interval\nIf the population follows a normal distribution or if the sample sizes n_1 and n_2 are sufficiently large so that n_1 \\hat{p}_1 \\geq 10 and n_1 (1 - \\hat{p}_1) \\geq 10 and that n_2 \\hat{p}_2 \\geq 10 and n_2 (1 - \\hat{p}_2) \\geq 10 for the sample proportions \\hat{p}_1 and \\hat{p}_2, then a confidence interval for the difference in population proportions p_1 - p_2 can be calculated as: (\\hat{p}_1 - \\hat{p}_2) \\pm z^* \\sqrt{\\dfrac{\\hat{p}_1 (1 - \\hat{p}_1)}{n_1} + \\dfrac{\\hat{p}_2 (1 - \\hat{p}_2)}{n_2}} where z^* is chosen such that P(-z^* &lt; Z &lt; z^*) = 1 - \\alpha for a given significance level \\alpha.\n\nExample\nSuppose we are given sample sizes of n_1 = 400 and n_2 = 600, sample proportions of \\hat{p}_1 = 0.84 and \\hat{p}_2 = 0.78, and a significance level of \\alpha = 0.05 (or a 1 - \\alpha = 0.95 confidence level).\nWithout knowing whether the populations follow a normal distribution, we verify that n_1 \\hat{p}_1 = (400)(0.84) = 336 \\geq 10, n_1 (1 - \\hat{p}_1) = (400)(0.16) = 64 \\geq 10, n_2 \\hat{p}_2 = (600)(0.78) = 468 \\geq 10, and n_2 (1 - \\hat{p}_2) = (600)(0.22) = 132 \\geq 10 are satisfied. Therefore, we can proceed with calculating the confidence interval:\n\nn1 &lt;- 400\nn2 &lt;- 600\np1_hat &lt;- 0.84\np2_hat &lt;- 0.78\nalpha &lt;- 0.05\n\n1z_star &lt;- qnorm(1 - (alpha / 2))\nstandard_error &lt;- sqrt(\n    ((p1_hat * (1 - p1_hat)) / n1) + ((p2_hat * (1 - p2_hat)) / n2)\n)\n2interval &lt;- (p1_hat - p2_hat) + c(-1, 1) * z_star * standard_error\n\ncat(\"(\", round(interval[1], 4), \", \", round(interval[2], 4), \")\", sep = \"\")\n\n\n1\n\nDivide alpha by 2 to determine the area for one tail, then subtract it from 1 to obtain the area up to the right tail. Then find the z-score that corresponds to this area using the qnorm function.\n\n2\n\nMultiplying the critical value by c(-1, 1) is just a lazy yet convenient way to calculate the lower and upper bounds of the confidence interval simultaneously.\n\n\n\n\n(0.0111, 0.1089)"
  },
  {
    "objectID": "proj/statistical-inference/index.html#chi-square-goodness-of-fit",
    "href": "proj/statistical-inference/index.html#chi-square-goodness-of-fit",
    "title": "Statistical Inference in Nutshell",
    "section": "Chi-Square Goodness of Fit",
    "text": "Chi-Square Goodness of Fit"
  },
  {
    "objectID": "proj/statistical-inference/index.html#chi-square-test-for-association",
    "href": "proj/statistical-inference/index.html#chi-square-test-for-association",
    "title": "Statistical Inference in Nutshell",
    "section": "Chi-Square Test for Association",
    "text": "Chi-Square Test for Association"
  },
  {
    "objectID": "proj/statistical-inference/index.html#single-mean",
    "href": "proj/statistical-inference/index.html#single-mean",
    "title": "Statistical Inference in Nutshell",
    "section": "Single Mean",
    "text": "Single Mean"
  },
  {
    "objectID": "proj/statistical-inference/index.html#difference-in-means",
    "href": "proj/statistical-inference/index.html#difference-in-means",
    "title": "Statistical Inference in Nutshell",
    "section": "Difference in Means",
    "text": "Difference in Means"
  },
  {
    "objectID": "proj/statistical-inference/index.html#paired-difference-in-means",
    "href": "proj/statistical-inference/index.html#paired-difference-in-means",
    "title": "Statistical Inference in Nutshell",
    "section": "Paired Difference in Means",
    "text": "Paired Difference in Means"
  },
  {
    "objectID": "proj/statistical-inference/index.html#analysis-of-variance-anova",
    "href": "proj/statistical-inference/index.html#analysis-of-variance-anova",
    "title": "Statistical Inference in Nutshell",
    "section": "Analysis of Variance (ANOVA)",
    "text": "Analysis of Variance (ANOVA)"
  },
  {
    "objectID": "proj/statistical-inference/index.html#correlation-simple-regression",
    "href": "proj/statistical-inference/index.html#correlation-simple-regression",
    "title": "Statistical Inference in Nutshell",
    "section": "Correlation, Simple Regression",
    "text": "Correlation, Simple Regression"
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html",
    "href": "proj/inspirative-text-prediction/index.html",
    "title": "Inspirative Text Prediction",
    "section": "",
    "text": "GitHub Copilot text completion system.\n\n\nMachine learning, and more specifically, deep learning, is shaping how we write. From academic papers to class materials, emails to text messages, we are constantly using technologies powered by deep learning to compose our texts. Moreover, studies have shown that predictive text influences what we write [1,2,4]. Currently, most text prediction technology uses a model that looks at the previously typed words and the surrounding text to generate a list of likely next words or phrases. It ranks each of them based on their probabilities and presents the most likely ones to users as suggestions. However, not only may those suggestions be biased, but they may also affect how users write and what they write, thereby taking away their authorship and autonomy. Could text prediction models instead serve as a source of inspiration for users, encouraging their writing process instead of suggesting what to write?\nIn this blog post, I will explore the possibility of using text prediction to inspire users to write more original texts. I will define what it means to be inspirational and then present a preliminary approach to collecting example data and evaluating the current large language models (LLMs) to determine their likelihood of predicting subordinating conjunctions. I will then discuss the challenges and opportunities of using text prediction to inspire users to write more original texts."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#introduction",
    "href": "proj/inspirative-text-prediction/index.html#introduction",
    "title": "Inspirative Text Prediction",
    "section": "",
    "text": "GitHub Copilot text completion system.\n\n\nMachine learning, and more specifically, deep learning, is shaping how we write. From academic papers to class materials, emails to text messages, we are constantly using technologies powered by deep learning to compose our texts. Moreover, studies have shown that predictive text influences what we write [1,2,4]. Currently, most text prediction technology uses a model that looks at the previously typed words and the surrounding text to generate a list of likely next words or phrases. It ranks each of them based on their probabilities and presents the most likely ones to users as suggestions. However, not only may those suggestions be biased, but they may also affect how users write and what they write, thereby taking away their authorship and autonomy. Could text prediction models instead serve as a source of inspiration for users, encouraging their writing process instead of suggesting what to write?\nIn this blog post, I will explore the possibility of using text prediction to inspire users to write more original texts. I will define what it means to be inspirational and then present a preliminary approach to collecting example data and evaluating the current large language models (LLMs) to determine their likelihood of predicting subordinating conjunctions. I will then discuss the challenges and opportunities of using text prediction to inspire users to write more original texts."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#background-and-related-work",
    "href": "proj/inspirative-text-prediction/index.html#background-and-related-work",
    "title": "Inspirative Text Prediction",
    "section": "Background and Related Work",
    "text": "Background and Related Work\n\nWhat does it mean to be inspirative?\nIn this blog post, I will use the term inspirative to describe the tendency “to draw forth or bring out.”1 Specifically, in the context of text prediction, this means that the model should inspire users to write original texts that are not generated by machines. Instead of suggesting the most likely next words or phrases, the model should encourage users to think about what they have written so far and what they could write next. Flower and Hayes define writing as a cognitive process, suggesting that it involves organizing and connecting various types of thinking processes that go into writing, each capable of interrupting and influencing the others [3]. How can we support this cognitive process in writing using text prediction? One possible approach investigated by Hyechan Jun, Ha-Ram Koo, and Advait Scaria involves presenting the prediction output in the form of an interview question rather than written text [5]. Questions inherently stimulate thinking and reflection, and this approach has the potential to prompt writers to think about their writing goals and evaluate their written text in order to answer the questions. However, a challenge is that questions must be thought-provoking and they must produce a new concept or idea to warrant their usefulness. In this work, I will explore a different approach. Since text prediction models are already good at predicting texts ahead of time, what if some of those texts could be hidden from the user, inspiring them to complete the text with existing hints?\n\n\nOn Subordinating Conjunctions\nSuppose a writer writes an independent clause: “The plant grew taller.” Instead of suggesting the most likely next words or phrases, the model could predict the word “because,” a subordinating conjunction. The word “because” has a fascinating property of making the writer think: “Why did the plant grow taller?” The writer could then complete the sentence with a dependent clause: “because they received an adequate amount of sunlight.” This approach to text prediction has the potential to inspire writers to think about their writing goals and evaluate their written text in order to complete their text with some hint, in this case, a subordinating conjunction. In this work, I will explore this approach by evaluating the current LLMs to determine their likelihood of predicting such subordinating conjunctions."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#exploratory-data-analysis",
    "href": "proj/inspirative-text-prediction/index.html#exploratory-data-analysis",
    "title": "Inspirative Text Prediction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nimport plotly.io as pio\nimport plotly.express as px\nimport pandas as pd\nimport requests\nimport spacy\n\npio.templates.default = \"plotly_white\"\n\nspacy.prefer_gpu()\nnlp = spacy.load(\"en_core_web_sm\")\n\nCollection\nLet’s start by defining a function to download a book from Project Gutenberg. To accomplish this, we will use Gutendex to retrieve the book’s metadata and then download the book using the URL to the plain text version of the book provided in the metadata. For the purpose of this blog post, we will only download books in English.\n#| code-fold: True\n\ndef download_book(book_id: int) -&gt; tuple[str, str]:\n    \"\"\"Download a book from Project Gutenberg\n\n    Arg:\n        book_id: The Project Gutenberg ID of the book to download\n\n    Returns:\n        A tuple containing the book title and the book text\n    \"\"\"\n\n    gutendex_url = f\"https://gutendex.com/books/{book_id}/\"\n\n    try:\n        response = requests.get(gutendex_url)\n        response.raise_for_status()\n        data = response.json()\n\n        book_language = data[\"languages\"]\n\n        # Only download books in English\n        if \"en\" in book_language:\n            book_title = data[\"title\"]\n\n            # Only download books in plain text\n            mime_types = [\"text/plain\", \"text/plain; charset=us-ascii\"]\n\n            for mime_type in mime_types:\n                if mime_type in data[\"formats\"]:\n                    book_url = data[\"formats\"][mime_type]\n                    break\n\n            if book_url is None:\n                raise Exception(\"The book is not available in plain text.\")\n\n            response = requests.get(book_url)\n            response.raise_for_status()\n\n            return book_title, response.text\n        else:\n            raise Exception(\"The book is not in English.\")\n    except requests.exceptions.HTTPError as err:\n        raise Exception(err)\nFor this EDA, we will download The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson.\n# Book ID for The Strange Case of Dr. Jekyll and Mr. Hyde\nbook_id = 43\n\n# Download the book and store it in a DataFrame\nbook_data = [download_book(book_id)]\nbook_data = pd.DataFrame(book_data, columns=[\"title\", \"text\"])\n\n\nWrangling\nLet’s take a look at the downloaded text:\n#| echo: false\n\n# Print the first 256 characters of the book\nprint(book_data[\"text\"].iloc[0][:256].strip() + \"\\n\\n...\\n\")\n\n# Print the last 256 characters of the book\nprint(book_data[\"text\"].iloc[0][-256:].strip(), end=\"\")\nIt looks like the text contains some extra information which we do not wish to include in our analysis. Let’s remove the extra information and save the cleaned text in a new column.\nSpecifically, we will use the markers provided by Project Gutenberg to remove the extra information. These markers appear as follows:\n\n*** START OF THE PROJECT GUTENBERG EBOOK …\n\n\n*** END OF THE PROJECT GUTENBERG EBOOK …\n\n#| code-fold: true\n\ndef sanitize_text(text: str) -&gt; str:\n    \"\"\"Remove extra information from the text\n\n    Arg:\n        text: The text to sanitize\n\n    Returns:\n        The sanitized text\n    \"\"\"\n\n    start_marker = \"***\"\n    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n\n    # Index of the second occurrence of the start marker\n    start_index = text.find(start_marker, text.find(start_marker) + 1)\n\n    # Index of the first occurrence of the end marker\n    end_index = text.find(end_marker)\n\n    # Remove the extra information based on the marker indices\n    if start_index != -1 and end_index != -1:\n        text = text[start_index + len(start_marker) : end_index].strip()\n\n    return text\n# Sanitize the text and store it in a new column\nbook_data[\"clean_text\"] = book_data[\"text\"].apply(sanitize_text)\nLet’s take a look at the cleaned text:\n#| echo: false\n\n# Print the first 256 characters of the book\nprint(book_data[\"clean_text\"].iloc[0][:256].strip() + \"\\n\\n...\\n\")\n\n# Print the last 256 characters of the book\nprint(book_data[\"clean_text\"].iloc[0][-256:].strip())\nThis looks much better! Our next step is to split the text into sentences to analyze it at the sentence level. We will use spaCy to do this:\n#| code-fold: true\n\ndef sentence_spliter(text: str) -&gt; list[str]:\n    \"\"\"Split the text into sentences\n\n    Arg:\n        text: The text to split\n\n    Returns:\n        A list of sentences\n    \"\"\"\n\n    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n\n    # Remove line breaks and split the text into sentences\n    doc = nlp.pipe([text.replace(\"\\r\\n\", \" \")], disable=pipe_disable)\n\n    # Return a list of sentences without leading and trailing whitespace\n    return [sent.text.strip() for doc in doc for sent in doc.sents]\n# Split the text into sentences and store them in a DataFrame\nsentences = sentence_spliter(book_data[\"clean_text\"].iloc[0])\nsentences = pd.DataFrame(sentences, columns=[\"sentence\"])\n\nsentences.tail()\nHow many sentences are there in the book?\n#| echo: false\n\nprint(f\"There are {len(sentences)} sentences in the book.\")\nHow many sentence use subordinating conjunctions? In order to answer this question, we will use spaCy’s part-of-speech tagger to identify sentences that contain subordinating conjunctions:\n#| code-fold: true\n\ndef doc_pipe(sentence: str):\n    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n    return list(nlp.pipe([sentence], disable=pipe_disable))\n\n\ndef has_sconj(sentence: str):\n    \"\"\"Check if a sentence contains a subordinating conjunction\n\n    Arg:\n        sentence: The sentence to check\n\n    Returns:\n        A Pandas Series containing a boolean value indicating whether the sentence contains a subordinating conjunction and the subordinating conjunction if it exists\n    \"\"\"\n\n    doc = doc_pipe(sentence)\n\n    # Check if the sentence contains a subordinating conjunction\n    for token in doc[0]:\n        if token.pos_ == \"SCONJ\":\n            return pd.Series([True, token.text])\n\n    return pd.Series([False, None])\n#| code-overflow: wrap\n\n# Check if the sentence contains a subordinating conjunction and store the result in a new column\nsentences[[\"has_sconj\", \"sconj\"]] = sentences[\"sentence\"].apply(has_sconj)\n\n# Sanity check\nassert sentences[\"has_sconj\"].value_counts().sum() == len(sentences)\n\nsentences.tail()\nHow many of the sentences contain subordinating conjunctions? How many of the sentences do not contain subordinating conjunctions?\n#| echo: false\n\nprint(\n    f\"There are {len(sentences[sentences['has_sconj']])} sentences with a subordinating conjunction,\\nand {len(sentences[~sentences['has_sconj']])} sentences without a subordinating conjunction.\"\n)\n\n\nVisualization\nLet’s try visualizing one of the sentences that contains a subordinating conjunction:\nFigure 1. Visualization of a Sentence That Contains a Subordinating Conjunction\n#| code-fold: true\n\n# Grab a sentence that contains a subordinating conjunction\nsentence_id = 1149\ndoc = nlp(sentences[\"sentence\"].iloc[sentence_id])\n\n# Visualize the sentence using displaCy\nspacy.displacy.render(doc, style=\"dep\", jupyter=True, options={\"distance\": 110})\nWhat about the distribution of subordinating conjunctions in the book?\n#| code-fold: true\n\n# Lower case the subordinating conjunctions and count them\nsent_sconj = sentences[\"sconj\"].str.lower().value_counts().reset_index()\n\n# Plot the distribution of subordinating conjunctions\nfig = px.bar(\n    sent_sconj,\n    x=\"sconj\",\n    y=\"count\",\n    title=\"&lt;b&gt;Figure 2.&lt;/b&gt; Distribution of Subordinating Conjunctions\",\n    labels={\"sconj\": \"Subordinating Conjunction\", \"count\": \"Count\"},\n    color_discrete_sequence=px.colors.qualitative.Safe\n)\n\nfig.show()\n\n\nAnalysis\nThis result is somewhat surprising to me. I did not expect “that” to be the most common subordinating conjunction in the book. I had expected “because” to be more common when compared to the other subordinating conjunctions since I personally use “because” frequently in my writing. This might suggest that there could be a different distribution of subordinating conjunctions that are more commonly used based on the writing context. Furthermore, this result does not provide any information about which subordinating conjunctions are more useful than others, particularly in the context of text prediction. Our next step is to evaluate the current large language models (LLMs) to determine their likelihood of predicting subordinating conjunctions."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#preliminary-modeling",
    "href": "proj/inspirative-text-prediction/index.html#preliminary-modeling",
    "title": "Inspirative Text Prediction",
    "section": "Preliminary Modeling",
    "text": "Preliminary Modeling\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.nn.functional import softmax, cross_entropy\nfrom datasets import load_dataset\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport spacy\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nspacy.prefer_gpu()\nnlp = spacy.load(\"en_core_web_sm\")\n\nLoad the Model\nWe will use the Llama-2-7b-chat-hf model to evaluate an LLM’s likelihood of predicting subordinating conjunctions. Unfortunately, running the model is computationally expensive on most machines. Therefore, we used AutoAWQ to quantize the model into 4-bit precision2. This reduces the amount of computational resources required to run inference on the model while still maintaining a high level of accuracy. We have provided our code for quantizing the model in the Appendix. In the meantime, you can access our quantized model here: CalvinU/Llama-2-7b-chat-hf-awq.\nmodel_name = \"CalvinU/Llama-2-7b-chat-hf-awq\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n\nLoad the Data\nWe have also described in the Appendix a scalable approach to collecting and processing data from Project Gutenberg. In the meantime, you can access our dataset here: CalvinU/project-gutenberg.\ndataset_name = \"CalvinU/project-gutenberg\"\n\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset_df = pd.DataFrame(dataset)\nThe dataset contains 10 random books downloaded from Project Gutenberg. These books have already been sanitized and split into sentences based on their book_id and title. Therefore, each row in the dataset represents an ordered sentence from one of the books. Let’s take a look at the dataset:\ndataset_df.tail()\n\n\nWrangling\nSince we already have an ordered list of sentences, we can apply the same approach we used in the EDA section to identify sentences that contain subordinating conjunctions:\n#| code-fold: true\n\ndef doc_pipe(sentence: str):\n    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n    return list(nlp.pipe([sentence], disable=pipe_disable))\n\n\ndef has_sconj(sentence: str):\n    \"\"\"Check if a sentence contains a subordinating conjunction\n\n    Arg:\n        sentence: The sentence to check\n\n    Returns:\n        A Pandas Series containing a boolean value indicating whether the sentence contains a subordinating conjunction and the subordinating conjunction if it exists\n    \"\"\"\n\n    doc = doc_pipe(sentence)\n\n    # Check if the sentence contains a subordinating conjunction\n    for token in doc[0]:\n        if token.pos_ == \"SCONJ\":\n            return pd.Series([True, token.text])\n\n    return pd.Series([False, None])\n# Check if the sentence contains a subordinating conjunction and store the result in a new column\ndataset_df[[\"has_sconj\", \"sconj\"]] = dataset_df[\"sentence\"].apply(has_sconj)\n\n# Sanity check\nassert dataset_df[\"has_sconj\"].value_counts().sum() == len(dataset_df)\n\ndataset_df.tail()\nNumber of sentences and number of sentences that contain subordinating conjunctions for each book:\nsummary = dataset_df.groupby([\"book_id\", \"title\"], as_index=False).agg(\n    num_sents=(\"sentence\", \"count\"),\n    num_sconj=(\"has_sconj\", \"sum\"),\n)\n\nsummary.head(10)\n\n\nAnalysis\nSuppose the general structure of a sentence with a subordinating conjunction is:\n&lt;sentence-with-SCONJ&gt; ::= &lt;subordinate-clause&gt; &lt;independent-clause&gt; | \n                          &lt;independent-clause&gt; &lt;subordinate-clause&gt;\nNote that a &lt;subordinate-clause&gt; is a dependent clause that contains a subordinating conjunction and cannot stand alone as a sentence, while an &lt;independent-clause&gt; is a main clause that can stand alone as a sentence.\nIn order to evaluate the likelihood of an LLM predicting subordinating conjunctions, we will investigate the following behaviors:\n\nHow does the cross-entropy and perplexity change when we provide the context exactly as it appears in the book, versus when we randomly shuffle the context?\n\n\nAnd for each case, what is the probability spectrum at the subordinating conjunction? What is the cross-entropy and perplexity of the text after the subordinating conjunction?\n\n\nWhen Context Is Provided Exactly as It Appears in the Book\nTo get started, let’s select one of the books from the dataset titled _ The Adventures of a Dog, and a Good Dog Too_ by Alfred Elwes:\n#| code-fold: true\n\n# Book ID for The Adventures of a Dog, and a Good Dog Too\nbook_id = 20741\n\nselected_book = dataset_df[dataset_df[\"book_id\"] == book_id].reset_index(drop=True)\nselected_book.tail()\nHow many sentences are there in the book? How many of the sentences contain subordinating conjunctions?\n#| echo: false\n\nprint(\n    f\"There are {len(selected_book)} sentences in the book. There are {len(selected_book[selected_book['has_sconj']])} sentences with a subordinating conjunction\"\n)\nIt appears that this book uses a significant number of subordinating conjunctions! Let’s choose one of the last sentences that includes a subordinating conjunction and select a maximum of 100 sentences preceding it as the context:\nlast_sconj_index = selected_book[selected_book[\"has_sconj\"]].index[-3]\n\ncontext = selected_book.iloc[max(last_sconj_index - 100, 0) : last_sconj_index][\n    \"sentence\"\n].tolist()\n\ncontext = \" \".join(context)\n\nsentence = selected_book.iloc[last_sconj_index][\"sentence\"]\nLet’s take a small peek at the context:\n#| echo: false\n\n# Print the first 50 characters of the context\nprint(context[:100].strip() + \" ... \", end=\"\")\n\n# Print the last 50 characters of the context\nprint(context[-100:].strip(), end=\"\\n\\n\")\nLet’s take a look at the sentence with the subordinating conjunction:\n#| echo: false\n\n# Print the sentence\nprint(sentence)\nLet’s tokenize the context and the sentence, and then feed them into the model to get the predicted logits:\n#| code-fold: true\n\n# Tokenize the context (to be used later, not as an input sequence)\ncontext_tokenized = tokenizer(context, return_tensors=\"pt\").to(device)\ncontext_input_ids = context_tokenized.input_ids\n\n# Tokenize the sentence (to be used later, not as an input sequence)\nsentence_tokenized = tokenizer(sentence, return_tensors=\"pt\").to(device)\nsentence_input_ids = sentence_tokenized.input_ids\n\n# Tokenize the context and the sentence as an input sequence\nprompt_tokenized = tokenizer(context + sentence, return_tensors=\"pt\").to(device)\nprompt_input_ids = prompt_tokenized.input_ids\n\n# Get the predicted logits for the input sequence\nmodel_logits = model(prompt_input_ids).logits\n#| code-fold: true\n\n# Get the subordinating conjunction from the book\nsconj = selected_book.iloc[last_sconj_index][\"sconj\"]\n\n# Decode the context as a string, excluding the first token\ncontext_decoded = [tokenizer.decode(token) for token in context_input_ids[0]][1:]\n\n# Decode the sentence as a string, excluding the first token\nsentence_decoded = [tokenizer.decode(token) for token in sentence_input_ids[0]][1:]\n\n# Index of the subordinating conjunction token in the input sequence\nsconj_token_index = len(context_decoded) + sentence_decoded.index(sconj)\n\n# Index of the subordinating conjunction in the sentence (not the input sequence, but from the book)\nsconj_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(sconj)\n\n# Figure out which type of clause comes first\nif sconj not in sentence[:sconj_index]:\n    independent_clause = sentence[:sconj_index]\n    subordinate_clause = sentence[sconj_index:]\nelse:\n    independent_clause = sentence[sconj_index:]\n    subordinate_clause = sentence[:sconj_index]\n\n# Index of the independent clause in the sentence (not the input sequence, but from the book)\nindependent_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n    independent_clause\n)\n\n# Index of the subordinate clause in the sentence (not the input sequence, but from the book)\nsubordinate_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n    subordinate_clause\n)\n\n# Tokenize the independent clause\nindependent_clause_tokenized = tokenizer(\n    independent_clause, return_tensors=\"pt\"\n).to(device)\n\n# Tokenize the subordinate clause\nsubordinate_clause_tokenized = tokenizer(\n    subordinate_clause, return_tensors=\"pt\"\n).to(device)\nGiven that we have fed in the context exactly as it appears in the book, let’s take a look at the top k probability spectrum at the subordinating conjunction:\n#| code-fold: true\n\ndef probability_spectrum_at(logits, input_ids, i, k=6):\n    \"\"\"Given an input sequence, get the top k probability spectrum at the given index\n\n    Args:\n        logits: predicted logits for the input sequence\n        input_ids: input sequence token IDs\n        i: index to get the probability spectrum at\n        k: top k, default is 6\n\n    Returns:\n        A Pandas DataFrame containing the top k probability spectrum at the given index\n    \"\"\"\n\n    # Predicted logits for an input sequence, excluding the last element\n    adjusted_logits = logits[0, :-1]\n\n    # Input sequence, starting from the second element\n    adjusted_input_ids = input_ids[0, 1:]\n\n    # Get the probability distribution predicted by the model\n    probability_distribution = softmax(adjusted_logits[i], dim=0)\n\n    # Get the top k probabilities and their respective indices, default k=6\n    top_probability_distribution, top_indices = probability_distribution.topk(k)\n\n    # Get the top k probability spectrum as a DataFrame\n    probability_spectrum = pd.DataFrame(\n        {\n            \"token\": [tokenizer.decode(token) for token in top_indices.tolist()],\n            \"probability\": top_probability_distribution.tolist(),\n        }\n    )\n\n    # Decode the input sequence as a string\n    matching_token = tokenizer.decode(adjusted_input_ids[i])\n\n    # Highlight the matching string in the probability spectrum\n    def highlight_prompt_at(x):\n        if x[\"token\"] == matching_token:\n            return [\"background-color: #6495ED\"] * len(x)\n        else:\n            return [\"\"] * len(x)\n\n    return probability_spectrum.style.apply(highlight_prompt_at, axis=1)\n\n\ndef cross_entropy_at(logits, input_ids, i):\n    \"\"\"Given an input sequence, get the cross entropy at the given index\n\n    Args:\n        logits: predicted logits for the input sequence\n        input_ids: input sequence token IDs\n        i: index to get the cross entropy at\n\n    Returns:\n        The cross entropy at the given index\n    \"\"\"\n\n    # Predicted logits for an input sequence, excluding the last element\n    adjusted_logits = logits[0, :-1]\n\n    # Input sequence, starting from the second element\n    adjusted_input_ids = input_ids[0, 1:]\n\n    # Get the cross entropy per input sequence\n    cross_entropy_seq = cross_entropy(\n        adjusted_logits, adjusted_input_ids, reduction=\"none\"\n    )\n\n    return cross_entropy_seq[i].item()\n\n\ndef cross_entropy_per_token(logits, input_ids, matching_sequence_tokenized):\n    \"\"\"Given a matching sequence, get the cross entropy for each token in the matching sequence\n\n    Args:\n        logits: predicted logits for the input sequence\n        input_ids: input sequence token IDs\n        matching_sequence_tokenized: tokenized matching sequence\n\n    Returns:\n        A Pandas DataFrame containing the cross entropy for each token in the matching sequence\n    \"\"\"\n\n    # Predicted logits for an input sequence, excluding the last element\n    adjusted_logits = logits[0, :-1]\n\n    # Input sequence, starting from the second element\n    adjusted_input_ids = input_ids[0, 1:]\n\n    # Get the cross entropy per input sequence\n    cross_entropy_seq = cross_entropy(\n        adjusted_logits, adjusted_input_ids, reduction=\"none\"\n    )\n\n    # Decode the tokenized matching sequence as a string\n    matching_sequence_token = [\n        tokenizer.decode(token) for token in matching_sequence_tokenized.input_ids[0]\n    ]\n\n    # Decoded matching sequence token, starting from the second element\n    adjusted_matching_sequence_token = matching_sequence_token[1:]\n\n    return pd.DataFrame(\n        {\n            \"token\": adjusted_matching_sequence_token,\n            \"cross_entropy\": cross_entropy_seq[\n                -len(adjusted_matching_sequence_token) :\n            ].tolist(),\n        }\n    )\n#| code-fold: true\n\nprob_spectrum = probability_spectrum_at(\n    model_logits, prompt_input_ids, sconj_token_index\n)\n\nprob_spectrum\nLet’s also look at the cross-entropy and perplexity of the sentence with the subordinating conjunction:\n#| code-fold: true\n\n# Cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\nsentence_per_token_cross_entropy = cross_entropy_per_token(\n    model_logits, prompt_input_ids, sentence_tokenized\n)\n\n# Mean cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\nmean_sentence_cross_entropy = sentence_per_token_cross_entropy[\"cross_entropy\"].mean()\n\n# Perplexity of the sentence with the subordinating conjunction (entire input sequence)\nsentence_perplexity = np.exp(mean_sentence_cross_entropy)\n\n# Cross-entropy of the independent clause\nindependent_clause_per_token_cross_entropy = cross_entropy_per_token(\n    model_logits, prompt_input_ids, independent_clause_tokenized\n)\n\n# Mean cross-entropy of the independent clause\nmean_independent_clause_cross_entropy = independent_clause_per_token_cross_entropy[\n    \"cross_entropy\"\n].mean()\n\n# Perplexity of the independent clause\nindependent_clause_perplexity = np.exp(mean_independent_clause_cross_entropy)\n\n# Cross-entropy of the subordinate clause\nsubordinate_clause_per_token_cross_entropy = cross_entropy_per_token(\n    model_logits, prompt_input_ids, subordinate_clause_tokenized\n)\n\n# Mean cross-entropy of the subordinate clause\nmean_subordinate_clause_cross_entropy = subordinate_clause_per_token_cross_entropy[\n    \"cross_entropy\"\n].mean()\n\n# Perplexity of the subordinate clause\nsubordinate_clause_perplexity = np.exp(mean_subordinate_clause_cross_entropy)\n\n# Cross-entropy of the subordinating conjunction\nsubordinating_conjunction_cross_entropy = cross_entropy_at(\n    model_logits, prompt_input_ids, sconj_token_index\n)\n\n# Perplexity of the subordinating conjunction\nsubordinating_conjunction_perplexity = np.exp(subordinating_conjunction_cross_entropy)\n#| echo: false\n\n# Print in the order of the sentence structure\nif independent_clause_index &gt; subordinate_clause_index:\n    print(\"Structure:\")\n    print(\"\\t&lt;sentence-with-SCONJ&gt; ::= &lt;subordinate-clause&gt; &lt;independent-clause&gt;\")\n    print(\"\\nMetrics:\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; cross-entropy: {mean_sentence_cross_entropy}\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; perplexity:    {sentence_perplexity}\\n\")\n    print(f\"\\t&lt;subordinate-clause&gt; cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n    print(f\"\\t&lt;subordinate-clause&gt; perplexity:     {subordinate_clause_perplexity}\")\n    print(f\"\\t&lt;SCONJ&gt; cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n    print(f\"\\t&lt;SCONJ&gt; perplexity:                  {subordinating_conjunction_perplexity}\")\n    print(f\"\\t&lt;independent-clause&gt; cross-entropy:  {mean_independent_clause_cross_entropy}\")\n    print(f\"\\t&lt;independent-clause&gt; perplexity:     {independent_clause_perplexity}\")\nelse:\n    print(\"Structure:\")\n    print(\"\\t&lt;sentence-with-SCONJ&gt; ::= &lt;independent-clause&gt; &lt;subordinate-clause&gt;\")\n    print(\"\\nMetrics:\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; cross-entropy: {mean_sentence_cross_entropy}\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; perplexity:    {sentence_perplexity}\\n\")\n    print(f\"\\t&lt;independent-clause&gt; cross-entropy:  {mean_independent_clause_cross_entropy}\")\n    print(f\"\\t&lt;independent-clause&gt; perplexity:     {independent_clause_perplexity}\")\n    print(f\"\\t&lt;SCONJ&gt; cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n    print(f\"\\t&lt;SCONJ&gt; perplexity:                  {subordinating_conjunction_perplexity}\")\n    print(f\"\\t&lt;subordinate-clause&gt; cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n    print(f\"\\t&lt;subordinate-clause&gt; perplexity:     {subordinate_clause_perplexity}\")\n\n\nWhen Context Is Randomly Shuffled\nLet’s shuffle the context and feed it into the model to get the predicted logits:\nlast_sconj_index = selected_book[selected_book[\"has_sconj\"]].index[-3]\n\ncontext = selected_book.iloc[max(last_sconj_index - 100, 0) : last_sconj_index][\n    \"sentence\"\n].tolist()\n\nrandom.Random(42).shuffle(context)\n\ncontext = \" \".join(context)\n\nsentence = selected_book.iloc[last_sconj_index][\"sentence\"]\nLet’s take a small peek at the context:\n#| echo: false\n\n# Print the first 50 characters of the context\nprint(context[:100].strip() + \" ... \", end=\"\")\n\n# Print the last 50 characters of the context\nprint(context[-100:].strip(), end=\"\\n\\n\")\nLet’s take a look at the sentence with the subordinating conjunction:\n#| echo: false\n\n# Print the sentence\nprint(sentence)\nSame steps as before:\n#| code-fold: true\n\n# Tokenize the context (to be used later, not as an input sequence)\ncontext_tokenized = tokenizer(context, return_tensors=\"pt\").to(device)\ncontext_input_ids = context_tokenized.input_ids\n\n# Tokenize the sentence (to be used later, not as an input sequence)\nsentence_tokenized = tokenizer(sentence, return_tensors=\"pt\").to(device)\nsentence_input_ids = sentence_tokenized.input_ids\n\n# Tokenize the context and the sentence as an input sequence\nprompt_tokenized = tokenizer(context + sentence, return_tensors=\"pt\").to(device)\nprompt_input_ids = prompt_tokenized.input_ids\n\n# Get the predicted logits for the input sequence\nmodel_logits = model(prompt_input_ids).logits\n#| code-fold: true\n\n# Get the subordinating conjunction from the book\nsconj = selected_book.iloc[last_sconj_index][\"sconj\"]\n\n# Decode the context as a string, excluding the first token\ncontext_decoded = [tokenizer.decode(token) for token in context_input_ids[0]][1:]\n\n# Decode the sentence as a string, excluding the first token\nsentence_decoded = [tokenizer.decode(token) for token in sentence_input_ids[0]][1:]\n\n# Index of the subordinating conjunction token in the input sequence\nsconj_token_index = len(context_decoded) + sentence_decoded.index(sconj)\n\n# Index of the subordinating conjunction in the sentence (not the input sequence, but from the book)\nsconj_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(sconj)\n\n# Figure out which type of clause comes first\nif sconj not in sentence[:sconj_index]:\n    independent_clause = sentence[:sconj_index]\n    subordinate_clause = sentence[sconj_index:]\nelse:\n    independent_clause = sentence[sconj_index:]\n    subordinate_clause = sentence[:sconj_index]\n\n# Index of the independent clause in the sentence (not the input sequence, but from the book)\nindependent_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n    independent_clause\n)\n\n# Index of the subordinate clause in the sentence (not the input sequence, but from the book)\nsubordinate_clause_index = selected_book.iloc[last_sconj_index][\"sentence\"].find(\n    subordinate_clause\n)\n\n# Tokenize the independent clause\nindependent_clause_tokenized = tokenizer(\n    independent_clause, return_tensors=\"pt\"\n).to(device)\n\n# Tokenize the subordinate clause\nsubordinate_clause_tokenized = tokenizer(\n    subordinate_clause, return_tensors=\"pt\"\n).to(device)\nGiven that we have fed in a randomly shuffled context, let’s take a look at the top k probability spectrum at the subordinating conjunction:\nprob_spectrum = probability_spectrum_at(\n    model_logits, prompt_input_ids, sconj_token_index\n)\n\nprob_spectrum\nLet’s also look at the cross-entropy and perplexity of the sentence with the subordinating conjunction:\n#| code-fold: true\n\n# Cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\nsentence_per_token_cross_entropy = cross_entropy_per_token(\n    model_logits, prompt_input_ids, sentence_tokenized\n)\n\n# Mean cross-entropy of the sentence with the subordinating conjunction (entire input sequence)\nmean_sentence_cross_entropy = sentence_per_token_cross_entropy[\"cross_entropy\"].mean()\n\n# Perplexity of the sentence with the subordinating conjunction (entire input sequence)\nsentence_perplexity = np.exp(mean_sentence_cross_entropy)\n\n# Cross-entropy of the independent clause\nindependent_clause_per_token_cross_entropy = cross_entropy_per_token(\n    model_logits, prompt_input_ids, independent_clause_tokenized\n)\n\n# Mean cross-entropy of the independent clause\nmean_independent_clause_cross_entropy = independent_clause_per_token_cross_entropy[\n    \"cross_entropy\"\n].mean()\n\n# Perplexity of the independent clause\nindependent_clause_perplexity = np.exp(mean_independent_clause_cross_entropy)\n\n# Cross-entropy of the subordinate clause\nsubordinate_clause_per_token_cross_entropy = cross_entropy_per_token(\n    model_logits, prompt_input_ids, subordinate_clause_tokenized\n)\n\n# Mean cross-entropy of the subordinate clause\nmean_subordinate_clause_cross_entropy = subordinate_clause_per_token_cross_entropy[\n    \"cross_entropy\"\n].mean()\n\n# Perplexity of the subordinate clause\nsubordinate_clause_perplexity = np.exp(mean_subordinate_clause_cross_entropy)\n\n# Cross-entropy of the subordinating conjunction\nsubordinating_conjunction_cross_entropy = cross_entropy_at(\n    model_logits, prompt_input_ids, sconj_token_index\n)\n\n# Perplexity of the subordinating conjunction\nsubordinating_conjunction_perplexity = np.exp(subordinating_conjunction_cross_entropy)\n#| echo: false\n\n# Print in the order of the sentence structure\nif independent_clause_index &gt; subordinate_clause_index:\n    print(\"Structure:\")\n    print(\"\\t&lt;sentence-with-SCONJ&gt; ::= &lt;subordinate-clause&gt; &lt;independent-clause&gt;\")\n    print(\"\\nMetrics:\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; cross-entropy: {mean_sentence_cross_entropy}\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; perplexity:    {sentence_perplexity}\\n\")\n    print(f\"\\t&lt;subordinate-clause&gt; cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n    print(f\"\\t&lt;subordinate-clause&gt; perplexity:     {subordinate_clause_perplexity}\")\n    print(f\"\\t&lt;SCONJ&gt; cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n    print(f\"\\t&lt;SCONJ&gt; perplexity:                  {subordinating_conjunction_perplexity}\")\n    print(f\"\\t&lt;independent-clause&gt; cross-entropy:  {mean_independent_clause_cross_entropy}\")\n    print(f\"\\t&lt;independent-clause&gt; perplexity:     {independent_clause_perplexity}\")\nelse:\n    print(\"Structure:\")\n    print(\"\\t&lt;sentence-with-SCONJ&gt; ::= &lt;independent-clause&gt; &lt;subordinate-clause&gt;\")\n    print(\"\\nMetrics:\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; cross-entropy: {mean_sentence_cross_entropy}\")\n    print(f\"\\t&lt;sentence-with-SCONJ&gt; perplexity:    {sentence_perplexity}\\n\")\n    print(f\"\\t&lt;independent-clause&gt; cross-entropy:  {mean_independent_clause_cross_entropy}\")\n    print(f\"\\t&lt;independent-clause&gt; perplexity:     {independent_clause_perplexity}\")\n    print(f\"\\t&lt;SCONJ&gt; cross-entropy:               {subordinating_conjunction_cross_entropy}\")\n    print(f\"\\t&lt;SCONJ&gt; perplexity:                  {subordinating_conjunction_perplexity}\")\n    print(f\"\\t&lt;subordinate-clause&gt; cross-entropy:  {mean_subordinate_clause_cross_entropy}\")\n    print(f\"\\t&lt;subordinate-clause&gt; perplexity:     {subordinate_clause_perplexity}\")"
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#results-and-conclusion",
    "href": "proj/inspirative-text-prediction/index.html#results-and-conclusion",
    "title": "Inspirative Text Prediction",
    "section": "Results and Conclusion",
    "text": "Results and Conclusion\nOur analysis section demonstrates that the cross-entropy and perplexity of the sentence with the subordinating conjunction change based on the context provided to the model. Furthermore, we observed that the probability of the subordinating conjunction is also affected by the context. This suggests that the context provided to the model is important for predicting subordinating conjunctions. In other words, the context provided to the model can influence the likelihood of the model predicting subordinating conjunctions. Moreover, we have also observed that, despite the change in the context, the cross-entropy and perplexity around the subordinate clause did not change as much as around the independent clause. Although this warrants more thorough investigation, it suggests that there is a certain kind of subordinating conjunction that appears to be more useful, even to the LLM (as it was still likely to construct the same subordinate clause even with the contextual change)."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#limitations",
    "href": "proj/inspirative-text-prediction/index.html#limitations",
    "title": "Inspirative Text Prediction",
    "section": "Limitations",
    "text": "Limitations\nOur work is not without limitations. Firstly, we have only analyzed the LLM with one book, which is not representative of different kinds of writing contexts. Furthermore, our approach is currently only able to parse subordinate clauses that position the subordinating conjunction in the middle of the sentence. There are certain edge cases related to the positioning of the subordinating conjunctions that we have not considered."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#future-work",
    "href": "proj/inspirative-text-prediction/index.html#future-work",
    "title": "Inspirative Text Prediction",
    "section": "Future Work",
    "text": "Future Work\nA natural extension of this work is to evaluate the LLM’s likelihood of predicting subordinating conjunctions with a more diverse and representative sample of data. Furthermore, we can also evaluate the LLM’s likelihood of predicting other kinds of conjunctions, such as coordinating conjunctions. Moreover, we can also evaluate the LLM’s likelihood of predicting subordinating conjunctions in different kinds of writing contexts, such as academic writing instead of books. Another way to extend this work is to evaluate the relationship between the LLM’s hyperparameters and its likelihood of predicting subordinating conjunctions."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#appendix",
    "href": "proj/inspirative-text-prediction/index.html#appendix",
    "title": "Inspirative Text Prediction",
    "section": "Appendix",
    "text": "Appendix\n\nAutoAWQ Quantization\nIn this section, we have documented our approach to quantizing the Llama-2-7b-chat-hf model using AutoAWQ into 4-bit precision. This reduces the amount of computational resources required to run inference on the model while still maintaining a high level of accuracy.\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer, AwqConfig\nmodel_name = \"meta-llama/Llama-2-7b-chat-hf\"\nquantized_model_path = \"Llama-2-7b-chat-hf-awq\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoAWQForCausalLM.from_pretrained(model_name, **{\"low_cpu_mem_usage\": True})\n# Setup AutoAWQ quantization configuration\nquant_config = {\n    \"zero_point\": True,\n    \"q_group_size\": 128,\n    \"w_bit\": 4,\n    \"version\": \"GEMM\",\n}\n\n# Quantize the model\nmodel.quantize(tokenizer, quant_config=quant_config)\n# Setup Transformer compatible quantization configuration\nquantization_config = AwqConfig(\n    bits=quant_config[\"w_bit\"],\n    group_size=quant_config[\"q_group_size\"],\n    zero_point=quant_config[\"zero_point\"],\n    version=quant_config[\"version\"].lower(),\n).to_dict()\n\n# Pass the new quantization configuration to the model\nmodel.model.config.quantization_config = quantization_config\n\n# Save the quantized model weights\ntokenizer.save_pretrained(quantized_model_path)\nmodel.save_quantized(quantized_model_path)\nTo promote reproducibility of this work, we have uploaded our quantized model to Hugging Face repositories. You can access our quantized model here: CalvinU/Llama-2-7b-chat-hf-awq.\n\n\nScalable Data Collection\nIn the EDA, we have only looked at one book. However, in a language modeling task, we would likely need a sample of data that is diverse and representative of different kinds of writing. In this section, we have documented a scalable approach to collecting and processing data from Project Gutenberg.\nimport pandas as pd\nimport requests\nimport random\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\nHere are the functions we have used. All of them were already defined in the EDA section, except for download_books, which is a wrapper function for download_book that downloads multiple books instead of just one:\n\n\nCode\n\ndef download_book(book_id: int) -&gt; tuple[str, str]:\n    \"\"\"Download a book from Project Gutenberg\n\n    Arg:\n        book_id: The Project Gutenberg ID of the book to download\n\n    Returns:\n        A tuple containing the book title and the book text\n    \"\"\"\n\n    gutendex_url = f\"https://gutendex.com/books/{book_id}/\"\n\n    try:\n        response = requests.get(gutendex_url)\n        response.raise_for_status()\n        data = response.json()\n\n        book_language = data[\"languages\"]\n\n        # Only download books in English\n        if \"en\" in book_language:\n            book_title = data[\"title\"]\n\n            # Only download books in plain text\n            mime_types = [\"text/plain\", \"text/plain; charset=us-ascii\"]\n\n            for mime_type in mime_types:\n                if mime_type in data[\"formats\"]:\n                    book_url = data[\"formats\"][mime_type]\n                    break\n\n            if book_url is None:\n                raise Exception(\"The book is not available in plain text.\")\n\n            response = requests.get(book_url)\n            response.raise_for_status()\n\n            return book_title, response.text\n        else:\n            raise Exception(\"The book is not in English.\")\n    except requests.exceptions.HTTPError as err:\n        raise Exception(err)\n\n\ndef download_books(n: int) -&gt; list[tuple[int, str, str]]:\n    \"\"\"Download n books from Project Gutenberg\n\n    Arg:\n        n: The number of books to download\n\n    Returns:\n        A list of downloaded books\n    \"\"\"\n\n    max_book_count = requests.get(\"https://gutendex.com/books/\").json()[\"count\"]\n\n    books = []\n\n    i = 0\n    while i &lt; n:\n        book_id = random.randint(1, max_book_count)\n\n        try:\n            book_title, book_text = download_book(book_id)\n            books.append((book_id, book_title, book_text))\n            i += 1\n        except Exception as e:\n            continue\n\n    return books\n\n\ndef sanitize_text(text: str) -&gt; str:\n    \"\"\"Remove extra information from the text\n\n    Arg:\n        text: The text to sanitize\n\n    Returns:\n        The sanitized text\n    \"\"\"\n\n    start_marker = \"***\"\n    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n\n    # Index of the second occurrence of the start marker\n    start_index = text.find(start_marker, text.find(start_marker) + 1)\n\n    # Index of the first occurrence of the end marker\n    end_index = text.find(end_marker)\n\n    # Remove the extra information based on the marker indices\n    if start_index != -1 and end_index != -1:\n        text = text[start_index + len(start_marker) : end_index].strip()\n\n    return text\n\n\ndef sentence_spliter(text: str) -&gt; list[str]:\n    \"\"\"Split the text into sentences\n\n    Arg:\n        text: The text to split\n\n    Returns:\n        A list of sentences\n    \"\"\"\n\n    nlp.max_length = len(text)\n\n    pipe_disable = [\"ner\", \"lemmatizer\", \"textcat\"]\n\n    # Remove line breaks and split the text into sentences\n    doc = nlp.pipe([text.replace(\"\\r\\n\", \" \")], disable=pipe_disable)\n\n    # Return a list of sentences without leading and trailing whitespace\n    return [sent.text.strip() for doc in doc for sent in doc.sents]\n\nDownload 10 random books from Project Gutenberg:\nn_books = 10\n\nbooks10 = pd.DataFrame(\n    download_books(n_books), \n    columns=[\"book_id\", \"title\", \"text\"]\n)\n\nassert len(books10) == n_books\nClean the texts:\nbooks10[\"clean_text\"] = books10[\"text\"].apply(sanitize_text)\nSplit the texts into sentences:\nbooks10_sentences = []\n\n# For each book, split the text into sentences\nfor i in range(0, len(books10)):\n    books10_sentences.append(\n        (\n            books10[\"book_id\"].iloc[i],\n            books10[\"title\"].iloc[i],\n            sentence_spliter(books10[\"clean_text\"].iloc[i]),\n        )\n    )\nCreate a new DataFrame with the sentences:\n# For each sentences in each id, create a new row\nbooks10_sentences = [\n    (id, title, sent) for id, title, sents in books10_sentences for sent in sents\n]\n\nbooks10_sentences = pd.DataFrame(\n    books10_sentences, columns=[\"book_id\", \"title\", \"sentence\"]\n)\nTo promote reproducibility of this work, we have saved the data we have collected and processed using this approach as a parquet file. You can view and access our dataset here: CalvinU/project-gutenberg."
  },
  {
    "objectID": "proj/inspirative-text-prediction/index.html#footnotes",
    "href": "proj/inspirative-text-prediction/index.html#footnotes",
    "title": "Inspirative Text Prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.merriam-webster.com/dictionary/inspire↩︎\nChosen for its simplicity, however, other quantization methods, such as llama.cpp, will also likely work.↩︎"
  },
  {
    "objectID": "proj/transformer-visualized/index.html",
    "href": "proj/transformer-visualized/index.html",
    "title": "Transformer in a Nutshell",
    "section": "",
    "text": "🚧 Under Construction 🚧"
  },
  {
    "objectID": "proj/index.html",
    "href": "proj/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nInspirative Text Prediction\n\n\n\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nJiho Kim\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "privacy-policy/index.html",
    "href": "privacy-policy/index.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "My website uses Google Analytics1 to collect data about your interactions with the site to better understand visitor behavior and improve content. Google Analytics collects data such as your IP address, browser type, device information, and pages visited on the site.\n\n\n\nAnalytics: I use the information to analyze website traffic and enhance user experience.\nCustomization: To personalize content and features based on user preferences.\nPerformance: To monitor and improve website functionality.\n\n\n\n\nI do not sell or share personal data collected through Google Analytics with third parties except as required by law. Google may process data as described in their Privacy Policy.\n\n\n\n\nOpt-Out: You can prevent data collection by Google Analytics by installing the Google Analytics Opt-out Browser Add-on.\nCookies: You can manage cookie preferences through your browser settings.\n\n\n\n\nTo learn more about how Google handles data, visit their Data Protection Policies. By using my website, you consent to the use of Google Analytics as outlined in this policy."
  },
  {
    "objectID": "privacy-policy/index.html#footnotes",
    "href": "privacy-policy/index.html#footnotes",
    "title": "Privacy Policy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI do plan on migrating to a more privacy-respecting analytics tool, such as WP Statistics in the future.↩︎"
  },
  {
    "objectID": "pubs/ai4revision/index.html",
    "href": "pubs/ai4revision/index.html",
    "title": "AI for Revision",
    "section": "",
    "text": "We’re reimagining how AI can support the writing process while preserving the authenticity of human authorship. Building on our previous work with Textfocals1—an AI writing assistant that offers adaptive feedback through customizable prompts—we’re exploring new frontiers in human-AI writing collaboration.\nOur research investigates two critical questions at the intersection of AI and writing:\n\nWe’re examining the relationship between AI-assisted reflection and direct revision, seeking to understand which approaches best serve writers while maintaining their creative autonomy.\nWe’re exploring how different interaction modes—particularly speech versus text—affect the depth and quality of the writing reflection process.\n\nThis work aims to advance our understanding of how AI can enhance the writing experience while preserving what matters most: the writer’s sense of ownership, agency, and authentic creative expression. Our findings will contribute to the broader conversation about designing AI systems that truly serve human needs and creativity."
  },
  {
    "objectID": "pubs/ai4revision/index.html#vision",
    "href": "pubs/ai4revision/index.html#vision",
    "title": "AI for Revision",
    "section": "",
    "text": "We’re reimagining how AI can support the writing process while preserving the authenticity of human authorship. Building on our previous work with Textfocals1—an AI writing assistant that offers adaptive feedback through customizable prompts—we’re exploring new frontiers in human-AI writing collaboration.\nOur research investigates two critical questions at the intersection of AI and writing:\n\nWe’re examining the relationship between AI-assisted reflection and direct revision, seeking to understand which approaches best serve writers while maintaining their creative autonomy.\nWe’re exploring how different interaction modes—particularly speech versus text—affect the depth and quality of the writing reflection process.\n\nThis work aims to advance our understanding of how AI can enhance the writing experience while preserving what matters most: the writer’s sense of ownership, agency, and authentic creative expression. Our findings will contribute to the broader conversation about designing AI systems that truly serve human needs and creativity."
  },
  {
    "objectID": "pubs/ai4revision/index.html#team",
    "href": "pubs/ai4revision/index.html#team",
    "title": "AI for Revision",
    "section": "Team",
    "text": "Team\nJiho Kim is a senior computer science student who is spearheading this project to be submitted to the Department of Computer Science at Calvin University as a partial fulfillment of his BCS in Computer Science.\nPhilippe Laban is a senior research scientist at Microsoft Research, and Xiang ‘Anthony’ Chen is an associate professor of electrical and computer engineering and computer science at the University of California, Los Angeles. They are both outside collaborators co-advising Jiho on this project.\nKenneth C. Arnold is an assistant professor of computer science and data science at Calvin University who is serving as the primary advisor on this project."
  },
  {
    "objectID": "pubs/ai4revision/index.html#code",
    "href": "pubs/ai4revision/index.html#code",
    "title": "AI for Revision",
    "section": "Code",
    "text": "Code\nNot yet public."
  },
  {
    "objectID": "pubs/ai4revision/index.html#report",
    "href": "pubs/ai4revision/index.html#report",
    "title": "AI for Revision",
    "section": "Report",
    "text": "Report\n\nSenior Project Proposal.\nSenior Project Status Report."
  },
  {
    "objectID": "pubs/ai4revision/index.html#misc",
    "href": "pubs/ai4revision/index.html#misc",
    "title": "AI for Revision",
    "section": "Misc",
    "text": "Misc\nThis research is supported by NSF CRII award 2246145."
  },
  {
    "objectID": "pubs/ai4revision/index.html#footnotes",
    "href": "pubs/ai4revision/index.html#footnotes",
    "title": "AI for Revision",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTowards Full Authorship with AI: Supporting Revision with AI-Generated Views↩︎"
  },
  {
    "objectID": "pubs/towards-full-authorship-with-ai/index.html",
    "href": "pubs/towards-full-authorship-with-ai/index.html",
    "title": "Towards Full Authorship with AI: Supporting Revision with AI-Generated Views",
    "section": "",
    "text": "Textfocals user interface prototype implemented as Microsoft Word add-in\n[PDF]   [Poster]   [Slides]   [Presentation]"
  },
  {
    "objectID": "pubs/towards-full-authorship-with-ai/index.html#abstract",
    "href": "pubs/towards-full-authorship-with-ai/index.html#abstract",
    "title": "Towards Full Authorship with AI: Supporting Revision with AI-Generated Views",
    "section": "Abstract",
    "text": "Abstract\nLarge language models (LLMs) are shaping a new user interface (UI) paradigm in writing tools by enabling users to generate text through prompts. This paradigm shifts some creative control from the user to the system, thereby diminishing the user’s authorship and autonomy in the writing process. To restore autonomy, we introduce Textfocals, a UI prototype designed to investigate a human-centered approach that emphasizes the user’s role in writing. Textfocals supports the writing process by providing LLM-generated summaries, questions, and advice (i.e., LLM views) in a sidebar of a text editor, encouraging reflection and self-driven revision in writing without direct text generation. Textfocals’ UI affordances, including contextually adaptive views and scaffolding for prompt selection and customization, offer a novel way to interact with LLMs where users maintain full authorship of their writing. A formative user study with Textfocals showed promising evidence that this approach might help users develop underdeveloped ideas, cater to the rhetorical audience, and clarify their writing. However, the study also showed interaction design challenges related to document navigation and scoping, prompt engineering, and context management. Our work highlights the breadth of the design space of writing support interfaces powered by generative AI that maintain authorship integrity."
  }
]